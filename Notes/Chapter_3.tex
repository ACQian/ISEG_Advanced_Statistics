
\chapter{Sufficiency and Information}

\section{Sufficiency}

Consider a parametric model $\mcF = \{f(\cdot|\theta): \theta\in\Theta\}$ with $X_1,\ldots, X_n$ iid random sample of size $n$ extracted from $X$.
The goal is to use the information contained in $X_1,\ldots,X_n$ to produce inferential statements about the unknown parameter $\theta$.
When we compute statistics, i.e. functions of the random sample, we are summarizing the information contained in the random sample.
\opn{When can we be assured that in the process we are not losing any relevant information about the parameter?}{
    When all information that is lost is spurious, i.e. irrelevant for $\theta$.
}

\ex{Poisson}{
    $X | \lambda\sim Po(\lambda)$ where $\lambda > 0$. 
    We observe a random sample of size $n=2$ and we know that the observed value of the statistic $T=\sum_{i=1}^{n}X_i=31$.
    What can we say about the random sample $X_1, X_2$?\\

    We want the possibility of the sample given all the information that I have.
    \begin{align*}
        \frac{P(X_1=x_1, X_2=x_2|\lambda)}{P(T=31|\lambda)} & = \frac{\frac{e^{-\lambda}\lambda^{x_1}}{x_1!}\frac{e^{-\lambda}\lambda^{x_2}}{x_2!}}{\frac{e^{-2\lambda}(2\lambda)^{31}}{31!}}\\
        & =\frac{31!}{x_1!x_2!}\times\frac{\lambda^{x_1+x_2}}{\lambda^{31}}\times\left(\frac{1}{2}\right)^{31}\\
        & =\frac{31!}{(31-x_2)!x_2!}\times\left(\frac{1}{2}\right)^{x_2}\left(1-\frac{1}{2}\right)^{31-x_2}\\
        \Rightarrow\quad & X_2|T=31 \sim B(31,\frac{1}{2})
    \end{align*}
    We don't need to know $\lambda$.
}

\dfn[]{Sufficient statistics}{
    We say that a statistic $T$ is sufficient for $\mcF$ or for $\theta$, 
    if the conditional distribution of the random sample given the observed value of $T$ does not depend on the unkown parameter $\theta$ for all $\theta$.
}
What is essentially means is that $f(x_1,\ldots,x_n|\theta,t)$ does not depend on $\theta$.
\begin{align*}
    f(x_1,\ldots,x_n|\theta,t) & = \frac{f(x_1,\ldots,x_n,t|\theta)}{f_T(t,\theta)}\\
    & = \begin{cases*}
        \frac{f(x_1,\ldots,x_n|\theta)}{f_T(t,\theta)}\quad \text{if }T(x_1,\ldots,x_n)=t\\
        0\quad \text{otherwise}
    \end{cases*}
\end{align*}
\ex[]{Definition : Bernoulli}{
    $X|\theta\sim B(1,\theta)$, and $X_1,\ldots,X_n|\theta \overset{iid}{\sim} B(1,\theta)$\\

    Intuitively, the sequence of success and failure should not matter, only the proportion is important. 
    Thus, $T=\sum_{i=1}^{n}X_i$ should be sufficient for $\theta$.
    \begin{align*}
        f(x_1,\ldots,x_n|\theta,t) & = \frac{f(x_1,\ldots,x_n|\theta)}{f_T(t|\theta)} \quad\text{if }T(x_1,\ldots,x_n)=t, \sum x_i = t\\
        & =\frac{\prod_{i=1}^{n}f(x_i|\theta)}{f_T(t|\theta)}
    \end{align*}
    Because $T|\theta\sim B(n,\theta)$
    \begin{align*}
        f(x_1,\ldots,x_n|\theta,t) & = \frac{\prod_{i=1}^{n}\theta^{x_i}(1-\theta)^{1-x_i}}{\left(\begin{array}{c}n\\t\end{array}\right)\theta^t (1-\theta)^{n-t}}\quad \text{if }\sum x_i = t\\
        & =\frac{\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}}{\left(\begin{array}{c}n\\t\end{array}\right)\theta^t (1-\theta)^{n-t}}\\
        & = \frac{1}{\left(\begin{array}{c}n\\t\end{array}\right)}
    \end{align*}
}
The definition of sufficient statistic is not very useful to discover sufficient statistics.
Thus we need to rely on another theorem.
\thm[]{Halmos-Savage Factorization Criterion}{
    A statistic $T$ is sufficient for $\theta$ if and only if there are non-negative functions $g$ and $h$ such that
    \begin{itemize}
        \item $g$ depends on $\theta$ and on the random sample exclusively through the observed value of $T$.
        Meaning it depends on parameter and statistics.
        \item $h$ depends exclusively on the random sample.
        \item $f(x_1,\ldots,x_n|\theta) = g(T(x_1,\ldots,x_n);\theta)\times h(x_1,\ldots,x_n)$
    \end{itemize}
}
\begin{proof}
    $T$ is sufficient for $\theta \Leftrightarrow$ There are non-negative function of $g$ and $h$.\\

    We first start from left to right.\\

    If $T$ is sufficient for $\theta$, then
    \begin{equation*}
        f(x_1,\ldots,x_n|t,\theta) = \frac{f(x_1,\ldots,x_n|\theta)}{f_T(t|\theta)}\quad \text{does not depend on $\theta$}
    \end{equation*}
    meaning that
    \begin{equation*}
        f(x_1,\ldots,x_n|t) = \frac{f(x_1,\ldots,x_n|\theta)}{f_T(t|\theta)}\quad \text{if }T(x_1,\ldots,x_n)=t
    \end{equation*}
    so we get
    \begin{equation*}
        f(x_1,\ldots,x_n|\theta) = \underbrace{f_T(t|\theta)}_{g(t,\theta)}\underbrace{f(x_1,\ldots,x_n|t)}_{h(x_1,\ldots,x_n)}
    \end{equation*}

    Now going from right to left.\\

    For $X$ discrete, $T$ is discrete. And for simplification, $\tilde{x} = (x_1,\ldots,x_n)$.
    \begin{align*}
        f_T(t|\theta) & = P(T=t|\theta) = \sum_{\tilde{x}:T(\tilde{x})=t}f(\tilde{x}|\theta)\\
        & = \sum_{\tilde{x}:T(\tilde{x}=t)}g(t;\theta)h(\tilde{x})\\
        & = g(t;\theta)\sum_{\tilde{x}:T(\tilde{x})=t}h(\tilde{x})
    \end{align*}
    we now have
    \begin{align*}
        f(x_1,\ldots,x_n|t,\theta) & = \frac{f(x_1,\ldots,x_n|\theta)}{f_T(t|\theta)}\quad \text{if }T(\tilde{x})=t\\
        & = \frac{g(t,\theta)h(\tilde{x})}{g(t,\theta)\sum_{\tilde{x}:T(\tilde{x})=t}h(\tilde{x})}\\
        & = \frac{h(\tilde{x})}{\sum_{\tilde{x}:T(\tilde{x})=t}h(\tilde{x})} \quad\text{does not depend on $\theta$}
    \end{align*}
\end{proof}

\ex{Factorization criterion : Poisson}{
    Let $X|\lambda\sim Po(\lambda)$
    \begin{align*}
        f(x_1,\ldots,x_n|\lambda) & = \prod_{i=1}^{n}f(x_1|\lambda) = \prod_{i=1}^{n}e^{-\lambda}\frac{\lambda^{x_i}}{x_i!}\\
        & = e^{-n\lambda}\frac{\lambda^{\sum x_i}}{\prod x_i!}\\
        & = \underbrace{e^{-n\lambda}\lambda^{\sum x_i}}_{g(\sum x_i;\lambda)}\underbrace{\frac{1}{\prod x_i!}}_{h(\tilde{x})}
    \end{align*}
}
\ex[]{Factorization criterion : Uniform}{
    Let $X|\theta\sim U(0,\theta)$, $\theta > 0$.
    \begin{align*}
        f(x|\theta) & = \frac{1}{\theta}\quad ,0<x<\theta\\
        & = \frac{1}{\theta} \,I_{(0,\theta)}(x)
    \end{align*}
    with $I_A(x) = \begin{cases*}
        1\quad  ,x\in A\\
        0\quad  ,\text{otherwise}
    \end{cases*}$
    \begin{align*}
        f(\tilde{x}|\theta) & = \prod_{i=1}^{n}f(x_i|\theta) = \prod_{i=1}^{n}\frac{1}{\theta}\,I_{(0,\theta)}(x_i)\\
        & = \theta^{-n}\,I_{(0,\infty)}(x_{(1)})\,I_{(0,\theta)}(x_{(n)})\\
        & = \underbrace{\theta^{-n}I_{(0,\theta)}(x_{(n)})}_{g(x_{(n)})}\underbrace{I_{(0,\infty)}(x_{(1)})}_{h(\tilde{x})}
    \end{align*}
}
\ex[]{Factorization criterion : Shifted exponential}{
    Consider the shifted exponential distribution $f(x|\lambda,\delta) = \lambda e^{-\lambda(x-\delta)}\,I_{[\delta,\infty)}(x)$
    \begin{align*}
        f(\tilde{x}|\lambda,\delta) & = \prod_{i=1}^{n}f(x_i|\lambda,\delta) = \prod_{i=1}^{n} \lambda e^{-\lambda(x_i-\delta)}\,I_{[\delta,\infty)}(x_i)\\
        & = \underbrace{\lambda^n e^{-\lambda\sum x_i}e^{n\lambda\delta}\,I_{[\delta,\infty)}(x_{(1)})}_{g(\sum x_i, x_{(1)};\lambda,\delta)}\times \underbrace{1}_{h(\tilde{x})}
    \end{align*}
    By the factorization criterion, $(\sum x_i, x_{(1)})$ is sufficient for $(\lambda,\delta)$.
}
Two of things to notice regarding sufficient statistics.
\begin{itemize}
    \item There is always a sufficient statistic, it can be all the samples.
    \item Sufficient statistics are not unique.
\end{itemize}
\ex[]{Un-uniqueness of sufficient statistics : Poisson}{
    $X\sim Po(\lambda)$ where $t = \sum x_i$
    \begin{itemize}
        \item $f(\tilde{x}|\lambda) = e^{-n\lambda}\lambda^t\frac{1}{\prod x_i!} \Rightarrow T=\sum x_i$ is sufficient.
        \item $f(\tilde{x}|\lambda) = e^{-n\lambda}\lambda^{n\bar{x}}\frac{1}{\prod x_i!} \Rightarrow T=\bar{x}$ is sufficient.
        \item $f(\tilde{x}|\lambda) = e^{-n\lambda}\lambda^{T_1}\lambda^{T_2}\frac{1}{\prod x_i!} 
        \Rightarrow (T_1, T_2)$ is sufficient where $T_1 = \sum_{i=1}^{n_1}x_i$ and $T_2 = \sum_{i=n_1+1}^{n}x_i$.
    \end{itemize}
}

Another important concept is the \textbf{partition} induced in the sample space by a statistic.
\begin{itemize}
    \item Any statistics $T: \mcX \to \bbR^q$ induces a partition in the sample space $\mcX$ and the partitions do not intercept.
    \begin{center}
        \includegraphics[scale=0.5]{Images/5.png}
        $A + B + C + D = \mcX \Rightarrow T_1(\tilde{x}) = C \rightarrow \Pi = \{\mcX\}$
        \includegraphics[scale=0.5]{Images/6.png}
    \end{center}
    If a statistic takes 2 values : $T_1(\tilde{x}) = \begin{cases}
        c_1 &,\tilde{x}\in \mcX_1\\
        c_2 &,\tilde{x}\in \mcX_2
    \end{cases} \rightarrow \begin{cases}
        \mcX_1 \cup \mcX_2 = \mcX\\
        \mcX_1 \cap \mcX_2 = \varnothing\\
        \Pi = \{\mcX_1,\mcX_2\}
    \end{cases}$\\
    \includegraphics[scale=0.5]{Images/7.png}
    \item The finer the partition induced by $T$ in $\mcX$, the less information is lost; the smaller is the data reduction operated by $T$.
    \item A sufficient statistic operates a data reduction that does not involve loss of relevant information about the parameter; the partition it induces is also said to be sufficient.
\end{itemize}
\nt{
    When the partition is finer as it can be, 
    $T(\tilde{x}) = \tilde{x}$, $\displaystyle\Pi = \cup_{\tilde{x}\in\mcX}\{\tilde{x}\}$, 
    because no information will be lost.
}
\begin{itemize}
    \item The notion of partition is more general than that of statistic;
    different statistics can induce the same partition, in which case they are said to be equivalent, i.e. they are one-to-one.
    \item If the partition induced by $T$ is finer than the partition induced by $S$, then $S$ is a function of $T$.
    In that case, if $S$ is sufficient, so is $T$, that is\begin{equation*}
        \begin{rcases}
        S = h(T)\\
        S \:\text{sufficient}
    \end{rcases} \Rightarrow T \:\text{sufficient}
    \end{equation*}
    \begin{center}
        \includegraphics[scale=0.5]{Images/8.png} $S$ is a function of $T$
    \end{center}
    $\begin{cases}
        S \rightarrow \{\mcX_1,\mcX_2\}\\
        T \rightarrow \{\mcX_1, \mcX_{21}, \mcX_{22}\}
    \end{cases}$ If $S$ is sufficient, then $T$ is sufficient, because $T$ is finer than $S$ partition wise.
    \item However, if $T$ is sufficient and $S = h(T)$, it is not a given that $S$ is also sufficient unless $S$ is injective,
    in which case, $S$ and $T$ are equivalent.
    \item We are interested in finding statistics which are sufficient but operates the least data reduction.
    In other words, statistics that induce the coarsest partition that is still sufficient.
    \begin{center}
        \includegraphics[scale=0.5]{Images/9.png}
    \end{center}
    \textcolor{blue}{Point A} is what we want to find, the minimum sufficient statistic. 
    We lose information as we are moving to the right hand side. 
\end{itemize}

\dfn[]{Minimal sufficient statistic}{
    A statistic is said to be minimal sufficient for $\mcF$ if it is sufficient,
    and if $S$ is any other sufficient statistic, then $T = h(S)$ for some $h$.
}
Consider a binary relation in $\mcX$ defined by $\begin{cases}
    \tilde{y}\in\mcX\\
    \tilde{x}\in\mcX
\end{cases}$, $\tilde{y} \:\bmR\: \tilde{x}$.
\begin{equation*}
    f(\tdy|\theta) = c(\tdx, \tdy)f(\tdx|\theta)\Leftrightarrow\underbrace{\frac{f(\tdy|\theta)}{f(\tdx|\theta)}}_{\text{likelihood ratio}} = c(\tdx,\tdy)
\end{equation*}
where $c(\tdx,\tdy) > 0$ and does not depend on $\theta$.\\

This binary relation is an equivalence relation, that is, it is
\begin{itemize}
    \item symmetric: $\tdx\:\bmR\:\tdy \Leftrightarrow \tdy\:\bmR\:\tdx$
    \item reflexive: $\tdx\:\bmR\:\tdx$
    \item transitive: $\tdx\:\bmR\:\tdy$ and $\tdy\:\bmR\:\tdz\Rightarrow \tdx\:\bmR\:\tdz$
\end{itemize}
Hence, it induces a partition in $\mcX$ with parts
\begin{align*}
    \Pi_x & = \{y\in\mcX : y\:\bmR\:x\}\text{ for }x:f(x|\theta)>0 \text{ for some } \theta\in\Theta\\
    \Pi_0 & = \{y\in\mcX : f(y|\theta)=0\quad\forall\theta\in\Theta\}
\end{align*}
\begin{proof}
    Transitive\\

    Consider the case where
    \begin{align*}
        \tdx\:\bmR\:\tdy & \Rightarrow f(\tdy|\theta) = c_1f(\tdx|\theta)\\
        \tdy\:\bmR\:\tdz & \Rightarrow f(\tdy|\theta) = c_2f(\tdz|\theta)
    \end{align*}
    then we have
    \begin{equation*}
        c_1f(\tdx|\theta) = c_2f(\tdz|\theta) \Rightarrow f(\tdx|\theta) = \frac{c_2}{c_1}f(\tdz|\theta) \Rightarrow \tdx\:\bmR\:\tdz
    \end{equation*}
\end{proof}
\thm[]{Lehmann-Scheffe}{
    The partition with parts $\Pi_0$ and $\{\Pi_x\}$ described above is minimal sufficient, and any statistic which induces it is minimal sufficient. 
}
\begin{center}
    \includegraphics[scale=0.5]{Images/10.png} $\Pi = \{\Pi_1,\Pi_2,\Pi_3,\Pi_4\}$, $G(\tdx) = \tdx_i$ if $\tdx\in\Pi_i$
\end{center}
\begin{proof}
    Consider the statistic $G$ that to each $x\in\mcX$ associates a representative of the element of the partition to which it belongs
    \begin{equation*}
        x\in\mcX \mapsto x_{\Pi}\in\Pi_x = G(x)
    \end{equation*}
    \textbf{Proof that G is sufficient}
    \begin{gather*}
        \tdx\in\Pi_i \Rightarrow \tdx\:\bmR\:\tdx_i\\
        f(\tdx|\theta) = c(\tdx_i,\tdx)f(\tdx|\theta) \underset{G(\tdx)=\tdx_i}{=} \underbrace{c(\tdx_i,\tdx)}_{g(\tdx)}\underbrace{f(G(\tdx|\theta))}_{g(G;\theta)}\\
        \Rightarrow G \text{ is sufficient}
    \end{gather*}
    Now \textbf{proof that $G$ is a "grosser" partition}
    \begin{gather*}
        U \text{ is sufficient}\\
        \Pi_U = \{\Pi_x^*\},\quad
        \begin{rcases}
            \tdx\in\mcX\\
            \tdy\in\Pi_x^*
        \end{rcases}\rightarrow \tdx\in\Pi_i\\
        \text{since $U$ is sufficient,}\\
        f(\tdx|\theta) = g(U(\tdx);\theta)h(\tdx)\\
        f(\tdy|\theta) = g(U(\tdy);\theta)h(\tdy) = g(U(\tdx);\theta)h(\tdy)\Leftrightarrow\\
        h(\tdx)f(\tdy|\theta) = \underbrace{h(\tdx)g(U(\tdx);\theta)}_{f(\tdx|\theta)}h(\tdy)\Leftrightarrow\\
        f(\tdy|\theta) = f(\tdx|\theta)\frac{h(\tdy|\theta)}{h(\tdx|\theta)}\Rightarrow \tdy\:\bmR\:\tdx\Rightarrow U\text{ is a finer partition than }G\\
        \therefore G \text{ is the minimal sufficient statistic}
    \end{gather*}
\end{proof}

\ex[]{Minimal sufficient statistic : Normal}{
    Let $X_1,\ldots,X_n$ be a random sample from a $N(\mu,\sigma^2)$ population and let $\theta = (\mu,\sigma^2)$
    \begin{align*}
        f(\tdx|\theta) & = \prod_{i=1}^{n}f(x_i|\mu\sigma^2)\\
        & = \prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma^2}}\exp\{-\frac{1}{2\sigma^2}(x_i-\mu)^2\}\\
        & = (2\pi)^{-\frac{n}{2}}(\sigma^2)^{-\frac{n}{2}}\exp\{-\frac{1}{2\sigma^2}(\sum x_i^2-2n\bar{x}\mu+n\mu^2)\}\\
        & = (2\pi)^{-\frac{n}{2}}(\sigma^2)^{-\frac{n}{2}}\exp\{-\frac{1}{2\sigma^2}\sum x_i^2\}\exp\{\frac{n\bar{x}\mu}{\sigma^2}\}\exp\{-\frac{1}{2\sigma^2}n\mu^2\}
    \end{align*}
    Now using the \textbf{Lehmann-Scheffe theorem},
    \begin{align*}
        \frac{f(\tdy|\theta)}{f(\tdx|\theta)} 
        & = \frac{
            (2\pi)^{-\frac{n}{2}}(\sigma^2)^{-\frac{n}{2}}\exp\{-\frac{1}{2\sigma^2}\sum y_i^2\}\exp\{\frac{n\bar{x}\mu}{\sigma^2}\}\exp\{-\frac{1}{2\sigma^2}n\mu^2\}}{
                (2\pi)^{-\frac{n}{2}}(\sigma^2)^{-\frac{n}{2}}\exp\{-\frac{1}{2\sigma^2}\sum x_i^2\}\exp\{\frac{n\bar{x}\mu}{\sigma^2}\}\exp\{-\frac{1}{2\sigma^2}n\mu^2\}}\\
        & = \exp\{-\frac{1}{2\sigma^2}(\sum y_i^2-\sum x_i^2)\}\exp\{\frac{n\mu}{\sigma^2}(\bar{y}-\bar{x})\}\quad \text{does not depend on }\theta\\
        & \Leftrightarrow \begin{cases}
            \sum y_i^2 = \sum x_i^2\\
            \bar{y} = \bar{x}
        \end{cases}
    \end{align*}
    A minimal sufficient statistic is $(\bar{x},\sum x_i^2)$ or $(\bar{x}, s^2)$
}
\ex[]{Minimal sufficient statistic : Exponential}{
    Let $X_1,\ldots,X_n$ be a random sample from a $Ex(\lambda)$ population and let $\theta = \lambda$
    \begin{gather*}
        f(x|\lambda) = \lambda e^{-\lambda n}\, ,n>0\\
        f(\tdx|\lambda) = \prod_{i=1}^{n}\lambda e^{-\lambda n} = \lambda^n\exp\{-\lambda\sum x_i\}\\
        \frac{f(\tdy|\theta)}{f(\tdx|\theta)} = \frac{\lambda^n\exp\{-\lambda\sum y_i\}}{\lambda^n\exp\{-\lambda\sum x_i\}}
        = \exp\{-\lambda(\sum y_i-\sum x_i)\}
    \end{gather*}
    Does not depend on $\lambda$ if and only if $\sum y_i = \sum x_i \Leftrightarrow \bar{x}=\bar{y} \Rightarrow \bar{x}$ is minimal sufficient
}

\section{Ancilarity and completeness}
\dfn[]{Ancillary statistic}{
    A statistic $T$ is said to be ancillary if its sampling distribution does not depend on the unknown parameter $\theta$.
}
$T$ is ancillary if $f_T(t|\theta)$ does not depend on $\theta$.
\dfn[]{Location-scale family of distributions}{
    The location-scale family of distribution is composed by all the probability distributions such that
    the associated comulative distribution function is of the form
    \begin{equation*}
        G(x|\delta,\lambda) = G\left(\frac{x-\lambda}{\delta}\right)
    \end{equation*}
    where $G$ is a function that does not involve unknown parameters, $\lambda\in\bbR$ is the location parameter,
    and $\delta>0$ is the scale parameter.\\

    This family includes the location family ($\delta$ is known) and the scale family ($\lambda$ is known).
}

\ex[]{Location-scale family : Normal}{
    Suppose that $X\sim N(\mu, \sigma^2)$
    \begin{equation*}
        \frac{x-\mu}{\sigma} \sim N(0,1)
    \end{equation*}
    \begin{align*}
        f(x|\mu,\sigma^2)
        & = P(X\leq x | \mu\sigma^2)\\
        & = P\left(\frac{X-\mu}{\sigma}\leq\frac{x-\mu}{\sigma}|\mu,\sigma^2\right)\\
        & = \Phi\left(\frac{x-\mu}{\sigma}\right)
    \end{align*}
    where $G = \Phi, \lambda = \mu, \delta = \sigma$.\\

    The normal family of distributions is a member of the location-scale family with location parameter $\mu$ and scale parameter $\delta$.
}
\ex[]{Location-scale family : Uniform}{
    Suppose that $X\sim U(0,\theta)$, $\theta>0$, then we have
    \begin{equation*}
        f(x|\theta) = \frac{1}{\theta}\, I_{(0,\theta)}(x)
    \end{equation*}
    \begin{align*}
        F(x|\theta) 
        & = \frac{x}{\theta} \underbrace{I_{(0,\theta)}(x)}_{0<x<\theta\Leftrightarrow0<\frac{x}{\theta}<1}\\
        & = \frac{x}{\theta}\, I_{(0,1)}(\frac{x}{\theta})\\
        & = G\left(\frac{x}{\theta}\right)
    \end{align*}
    where $G(y) = y\,I_{(0,1)}(y)$\\

    $U(0,\theta)$ belongs to the scale family with scale parameter $\delta=\theta$ and $\lambda=0$.
}

Couple \textbf{remarks} regarding the location-scale family
\begin{itemize}
    \item The distribution of $X$ is part of the location-scale family with location parameter $\lambda$ and scale parameter $\delta$
    if and only if the distribution of $\frac{(x-\lambda)}{\delta}$ does not depend on unknown parameters.
    \item If the distribution of $X$ is a member of the location-scale family with location paratmer $\lambda$
    and scale parameter $\delta$ then any statistic which is a function of $X_1,\ldots,X_n$ only through the vector
    $\left(\frac{X_i-\lambda}{\delta}, i=1,\ldots,n\right)$ is ancillary.
    \begin{equation*}
        T = T(X_1,\ldots,X_n) = H\left(\frac{X_1-\lambda}{\delta},\ldots,\frac{X_n-\lambda}{\delta}\right),\quad T\text{ is ancillary}
    \end{equation*}
    \begin{proof}
        X belongs to the location-scale family
        \begin{flalign*}
            & \Leftrightarrow F(X) = P(X\leq x) = G\left(\frac{X-\lambda}{\delta}\right)\\
            & \Rightarrow P\left(\frac{X-\lambda}{\delta}\leq x\right) = P(X\leq\delta x+\lambda) = G\left(\frac{\delta x+\lambda-\lambda}{\delta}\right) = G(x)\to\text{ doesn't depend on }(\lambda,\delta) 
        \end{flalign*}
        Let $G$ be the CDF of $Y = \frac{X-\lambda}{\delta} \Leftrightarrow X = \delta Y+\lambda$
        \begin{equation*}
            P(X\leq x) = P(\delta Y + \lambda \leq x) = P\left(Y\leq \frac{x-\lambda}{\delta}\right) = G\left(\frac{x-\lambda}{\delta}\right)
        \end{equation*}
    \end{proof}
\end{itemize}

\ex[]{Location-scale and statistic : Uniform}{
    Let $X|\theta \sim U(\theta - \frac{1}{2}, \theta + \frac{1}{2})$, $\theta\in\bbR$
    \begin{equation*}
        f(x|\theta) = \frac{1}{1}\underbrace{I_{(\theta-\frac{1}{2},\theta+\frac{1}{2})}(x)}_{\theta-\frac{1}{2}<x<\theta+\frac{1}{2}\Leftrightarrow -\frac{1}{2}<x-\theta<\frac{1}{2}} 
        = I_{(-\frac{1}{2},\frac{1}{2})}(x-\theta) = G(x-\theta)
    \end{equation*}
    where $G(y) = I_{(-\frac{1}{2},\frac{1}{2})}(y)$\\

    The distribution of $X$ belongs to the location family with $\lambda = \theta$ and $\delta = 1$. 
    The distribution of $x-\theta$ does not depend on unknwon parameters.
    \begin{equation*}
        R = X_{(n)}-X_{(1)} = (X_{(n)}-\theta)-(X_{(1)}-\theta) = H(X_1-\theta_1,\ldots,X_n-\theta_n)
    \end{equation*}
}
\begin{itemize}
    \item It would be natural to expect a minimal sufficient statistic and an ancillary statistic to be indepenendent.
    However, that is not the case in general.
\end{itemize}
\ex[]{Minimal sufficient statistic and ancillary statistic : Uniform}{
    Let $X|\theta \sim U(\theta - \frac{1}{2}, \theta + \frac{1}{2})$
    \begin{equation*}
        \frac{f(\tdy|\theta)}{f(\tdx|\theta)} = \frac{I_{(y_{(n)}-\frac{1}{2},\infty)}(\theta)I_{(-\infty,y_{(1)}+\frac{1}{2})}(\theta)}{I_{(x_{(n)}-\frac{1}{2},\infty)}(\theta)I_{(-\infty,x_{(1)}+\frac{1}{2})}(\theta)}
    \end{equation*}
    does not depend on $\theta$ if and only if $\begin{cases}
        x_{(n)} = y_{(n)}\\
        x_{(1)} = y_{(1)}
    \end{cases} \Rightarrow (x_{(1)}, x_{(n)})$ is minimal sufficient, $(x_{(n)}-x_{(1)}, x_{(1)})$ is minimal sufficient.\\

    $(R, X_{(1)})$ is minimal sufficient but contains an element that is ancillary.
}
\dfn[]{Complete statistic}{
    A statistic $T$ is said to be complete if and only if 
    \begin{equation*}
        E[h(T)|\theta]=0\; \forall \theta\in\Theta \Rightarrow h(T)\equiv 0
    \end{equation*}
}
When $T$ is complete and $h_1(T)$ and $h_2(T)$ are two functions of $T$ that have the same expected value,
then it must be the case that they are the same, $h_1(T) = h_2(T)$.
\begin{proof}
    \begin{flalign*}
        & E_{\theta}[h_1(T)-h_2(T)] = 0\quad \forall\theta\in\Theta\\
        & \Downarrow\\
        & h_1(T)-h_2(T) = 0\\
        &\Updownarrow\\
        & h_1(T) = h_2(T)\quad \text{must be the same}
    \end{flalign*}
\end{proof}

\thm[]{}{Any statistic that is sufficient and complete is minimal sufficient.}
\begin{proof}
    Assume $T$ is sufficient and complete.
    \begin{flalign*}
        & \exists T_1\quad T_1=g(T),\quad T_1\text{ sufficient}\\
        & h(T) = T - \underbrace{E[T|T_1]}_{\text{function of $T_1$ because }T_1=g(T)}
    \end{flalign*}
    Conditional expectation: $E[X|Y=y] = \int_X x f_{X|Y=y}(x)\; dx$ which is a function of $y$.
    This expectation could be a function of the parameter, need to make sure that it's not the case for $h(T)$ to be a statistic.
    \begin{equation*}
        E[T|T_1=t_1] = \int_x T(\tdx)f_{\tdx|T_1=t_1,\theta}(\tdx)\;d\tdx
    \end{equation*}
    Since $T_1$ is sufficient, by definition, $f_{\tdx|T_1=t_1,\theta}(\tdx)$ does not depend on $\theta\Rightarrow E[T|T_1]$ does not depend on $\theta$.
    $\Rightarrow h(T)$ is a statistic.
    \begin{align*}
        E_{\theta}[h(T)] & = E_{\theta}[T]-\underbrace{E_{\theta}[E[T|T_1]]}_{\text{law of iterated expectation}}\\
        & = E_{\theta}[T] - E_{\theta}[T] = 0
    \end{align*}
    Since $T$ is complete, $h(T) = 0\Leftrightarrow T=E[T|T_1] = f(T_1)$
\end{proof}
\wc[]{Converse}{
    Let $X|\theta\sim U(\theta-\frac{1}{2},\theta+\frac{1}{2})$\\

    $R = X_{(n)} - X_{(1)}$ which is ancillary $\to$ distribution does not depend on $\theta$\\

    $T(R, X_{(1)})$ is minimal sufficient but $R$ has no information about $\theta$ so should throw away,
    but if you throw away then it is not minimal sufficient.
    \begin{equation*}
        h(T) = R - \underbrace{E[R]}_{= c\text{ because $R$ is ancillary}}
    \end{equation*}
    $E[h(T)] = 0$ but $h(T)\neq 0\Rightarrow T$ is not complete. 
}
Turns out that sufficient and complete statistics operate a data reduction that is more effective than that operated by minimal sufficient statistics that are not complete.
This is the subject of the Theorem of Basu.
\thm[]{Basu's Theorem}{
    Let $T$ be a sufficient and complete statistic. Then $T$ is independent of any ancillary statistic.
}
\begin{proof}
    Assume\begin{itemize}
        \item $T$ is sufficient and complete
        \item $U$ is ancillary
        \item $P(U\in A|T) = h_A(T)$ because conditioning on T
    \end{itemize}
    $h_A(T)$ is a statistic and $T$ is sufficient so does not depend on $\theta$.
    \begin{align*}
        P(U\in A|T=t) & = \int_x 1_A(U(\tdx))f_{\tdx|T=t,\theta}(\tdx)\, d\tdx\\
        & = \int_{\{\tdx:U(\tdx)\in A\}}U(\tdx)f_{\tdx|T=t,\theta}(\tdx)\,d\tdx\quad \text{where $T=t,\theta$ does not depend on $\theta$ because $T$ sufficient}
    \end{align*}
    but $E[h_A(T)]$ could depend on $\theta$ but does not because 
    \begin{flalign*}
        & h_A(T) = P(U\in A|T) = E[1_A(U)|T]\to \text{Bernoulli}\\
        & E[h_A(T)] = E[E[1_A(U)|T]] = E[1_A(U)] = \underbrace{P(U\in A)}_{\text{marginal probability}} = c\quad \text{does not depend on $\theta$, so a constant}\\
        & h_A(T) - c\quad \text{still a statistic because (statistic - constant) = statistic}\\
        & E[h_A(T) - c] = 0 \overset{T \text{ complete}}{\Longrightarrow} h_A(T) = c \overset{A \text{ arbitrary}}{\Longleftrightarrow} P(U\in A|T) = P(U\in A)
    \end{flalign*}
    $\Rightarrow$ $U$ and $T$ are independent.
\end{proof}
A couple \textbf{remarks}:
\begin{itemize}
    \item A complete and sufficient statistic does not contain any ancillary information, that is, 
    the data reduction operated by this type of statistics is more effective than that opearted by minimal sufficient statistics which are not complete.
    \item Either all the minimal sufficient statistics are complete or there are no sufficient and complete statistics.
    \begin{center}
        \includegraphics[scale=0.7]{Images/11.png}
    \end{center}
\end{itemize}

\section{Exponential family}
\dfn[]{Exponential family of distribution}{
    We say that a random vector $X$ is distributed according to a member of the k-parametric exponential family if its pdf or pmf can be expressed in the form 
    \begin{equation*}
        f(x|\theta) = c(\theta)\: h(x)\: \exp\left[\sum_{j=1}^{k}\omega_j(\theta)\: R_j(x)\right]
    \end{equation*}
    with support $\{x: f(x|\theta)>0\}$ independent of $\theta = (\theta_1,\ldots,\theta_k)$. Also, $c(\theta)\geq 0, h(x)\geq 0$, and $R_j(x)$ are scalar functions of $x$.
}
The \textbf{canonical form} of a distribution which belongs to the k-parameter exponential family is obtained through the so-called natural parametrization, 
$\alpha_j = \omega_j(\theta)$, $j = 1,\ldots,k$
\begin{equation*}
    f(x|alpha) = d(\alpha)\: h(x)\: \exp\left[\sum_{j=1}^{k}\alpha_j\: R_j(x)\right]
\end{equation*}
where $\alpha = (\alpha_1,\ldots,\alpha_k)\in A$ is called the natural parameter and $A$ being called the natural parameteric space.
\ex[]{Exponential family : Normal}{
    \begin{align*}
        f(X|\mu,\sigma^2) & = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{\frac{-1}{2\sigma^2}(X-\mu)^2\right\}\\
        & = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{\frac{-1}{2\sigma^2}(X^2-2\mu X-\mu^2)\right\}\\
        & = \underbrace{\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{\frac{-\mu X}{2\sigma^2}\right\}}_{c(\theta)}
        \exp\left\{\underbrace{\frac{-1}{2\sigma^2}}_{\omega_1(\theta)}\underbrace{X^2}_{R_1(X)}+\underbrace{\frac{\mu}{\sigma^2}}_{\omega_2(\theta)}\underbrace{X}_{R_2(X)}\right\}
    \end{align*}
    $\Rightarrow$ The Normal distribution belongs to the 2-parameters exponential family.\\

    Natural parameterization of the Normal distribution is 
    \begin{flalign*}
        & \alpha_1 = \frac{-1}{2\sigma^2}\quad \alpha_2 = \frac{\mu}{\sigma^2}\\
        & (\alpha_1,\alpha_2) \in \bbR\times\bbR\to \text{ the natural parameter space}
    \end{flalign*}
}
\ex[]{Exponential family : Poisson}{
    \begin{align*}
        f(x|\theta) & = e^{-\theta}\frac{\theta^x}{x!} = e^{-\theta}\frac{1}{x!}\exp(\ln\theta^x)\\
        & = \underbrace{e^{-\theta}}_{c(\theta)}\underbrace{\frac{1}{x!}}_{h(x)}\exp\left(\underbrace{\ln\theta}_{\omega_1(\theta)}\underbrace{x}_{R_1(x)}\right)
    \end{align*}
    $\Rightarrow$ The Poisson distribution is a member of the 1-parameter exponential family.
}
\thm[]{}{
    The k-parameter exponential family structure is preserved under iid random sampling and there is a k-dimentional sufficient statistic
    regardless of the sample size.
}
\begin{proof}
    $X_1,\ldots,X_n$ random sample from a model that is a member of the k-parameter exponential family.
    \begin{align*}
        f(\tdx|\tilde{\theta}) & = \prod_{i=1}^{n}f(x_i|\tilde{\theta}) = \prod_{i=1}^{n} c(\tilde{\theta}) h(x_i)\exp(\sum_{j=1}^{k}w_j(\theta)R_j(x_i))\\
        & = [c(\tilde{\theta})]^n \prod_{i=1}^{n}h(x_i)\exp(\sum_{i=1}^{n}\sum_{j=1}^{k}w_j(\theta)R_j(x_i))\\
        & = c^*(\theta)h^*(\tdx)\exp(\underbrace{\sum_{j=1}^{k}w_j(\theta)}_{w_j(\theta)}\underbrace{\sum_{i=1}^{n}R_j(x_i)}_{R^*(\tdx)=T_j(\tdx)})\\
        & = \underbrace{c^*(\theta)\exp(\sum_{j=1}^{k}w_j(\theta)T_j(\tdx))}_{g(\tdT;\theta)}\underbrace{h^*(\tdx)}_{h^*(\tdx)}
    \end{align*}
    $T = (T_1,\ldots,T_k)$ is a sufficient statistic by the factorization criterion.
\end{proof}
\thm[]{}{
    The sufficient statistic $T = (T_1(X),\ldots,T_k(X))$ just described is complete if the natural parametric space $A$ contains an open subset of $\bbR^k$
}
\includegraphics[scale=0.4]{Images/12.png} $\Rightarrow$ Contains an open subset of $\bbR^k$\\

\includegraphics[scale=0.4]{Images/13.png} $\Rightarrow$ Doesn't contain an open subset of $\bbR^k$

\ex[]{Sufficient, complete, and minimal sufficient : Normal}{
    \begin{flalign*}
        & \alpha_1 = \frac{-1}{\sigma^2} \in \bbR_{-}\\
        & \alpha_2 = \frac{\mu}{\sigma^2} \in \bbR
    \end{flalign*}
    \begin{center}
        \includegraphics[scale=0.4]{Images/14.png}
    \end{center}
    $\Rightarrow (T_1,T_2)$ is complete it was already sufficient $\Rightarrow (T_1,T_2)$ is sufficient and complete $\Rightarrow$ minimal sufficient
}
\ex[]{Basu : $\bar{X}, S^2$ independet in Normal}{
    Begin by assuming that the variance is known, $\sigma^2 = \sigma^2_0$\\

    $N(\mu,\sigma_0^2)$ belongs to the 1-parameter exponential family
    \begin{flalign*}
        & \exp\left\{
            \underbrace{\frac{-1}{2\sigma_0^2}x^2}_{\text{(all known)} x^2} + \frac{\mu}{\sigma_0^2}x
        \right\}\\
        & \alpha_1 = \frac{\mu}{\sigma_0^2}\quad T_1 = \sum x_i
    \end{flalign*}
    $\Rightarrow T_1$ is sufficient and complete $\Rightarrow \bar{X}$ is sufficient and complete\\

    And since $\sigma^2$ is known, the Normal distribution is also part of the location family with parameter $\mu$
    \begin{equation*}
        S^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i-\bar{x})^2 = \frac{1}{n}\sum_{i=1}^{n}((x_i-\mu)(\bar{x}-\mu))^2 = \frac{1}{n}\sum_{i=1}^{n}[(x_i-\mu)-\frac{1}{n}\sum_{i=1}^{n}(x_i-\mu)]^2 = (\cdots)
    \end{equation*}
    Essentially, $S^2 = H(x_i-\mu,\; i=1,\ldots,n)\Rightarrow S^2$ is ancillary 
    \begin{equation*}
        P(\bar{X}\leq t, S^2\leq s|\mu,\sigma_0^2) = P(\bar{X}\leq t|\mu,\sigma_0^2)P(S^2\leq s|\mu,\sigma_0^2)\quad \forall \sigma_0^2\to\text{ arbitrary}
    \end{equation*}
    $\Rightarrow \bar{X}$ and $S^2$ are independent in the Normal 2-parameter model
}
\clm[]{}{
    When we restrict a model, sufficiency is maintained, completeness not always.
}
\ex[]{Restricing a model : Normal}{
    Suppose that $X\sim N(\mu,\sigma^2)$ but consider a sub-family $\mu=\sigma^2$\\

    Clearly, $T=(S^2,\bar{X})$ is still sufficient for this sub-family. However, the natural parameter space no longer contains an open subset of $\bbR^2$ so that the previous theorem (3.3.2)
    does not guarantee the completeness of $T$.\\

    It is easy to see that $T$ is in this setting not complete as $\displaystyle\frac{\bar{X}-nS^2}{(n-1)}$ is a function of $T$ which has zero expected value but is obviously not null.
}

\section{Fisher information}
\dfn[]{Likelihood function}{
    When we observe the sample $(x_1,\ldots,x_n)$, the observed value of the random sample $X_1,\ldots,X_n$ with joint pdf or pmf function $f(x_1,\ldots,x_n)$,
    the corresponding likelihood function is a function of $\theta$ given by 
    \begin{equation*}
        L(\theta|x_1,\ldots,x_n) = f(x_1,\ldots,x_n|\theta)
    \end{equation*}
}
Essentially it shows how likely it is to observe the sample given the parameter. 
And since $x_1,\ldots,x_n$ is fixed, it's usual to write $L(\theta)$ instead of $L(\theta|x_1,\ldots,x_n)$. 
Note that $L$ is a function of $\theta$ and not of $x_1,\ldots,x_n$.
If \textbf{$X_1,\ldots,X_n$ is an iid random sample}, we have 
\begin{equation*}
    L(\theta|x_1,\ldots,x_n) = f(x_1,\ldots,x_n|\theta) = \prod_{i=1}^{n}f(x_i|\theta)
\end{equation*}
The likelihood function is not a probability mass or density function. Hence it has no natural scale associated.
In fact, it is more approriately defined as 
\begin{equation*}
    L(\theta|x_1\ldots,x_n) \propto f(x_1,\ldots,x_n|\theta)
\end{equation*}
that is, up to a multiplying constant. This multiplying constant can depend on $x_1,\ldots,x_n$ but not on $\theta$. 
\ex[]{Proportional : Poisson}{
    
}
What bears meaning





%\dfn{Definition Topic}{Definition Statement}
%\thm{Theorem Name}{Theorem Statement}
%\cor[cori]{Corollary Name}{Corollary Statement}
%\lem{Lemma Name}{Lemma Statement}
%\clm{Claim Name}{Claim Statement}
%\ex{Example Name}{Example explained}
%\opn{Open Question Name}{Question Statement}
%\pr{Question Name}{Question Statement}
%\nt{Special Note}
%\wc{Wrong Concept topic}{Explanation}
%\proof{Proof Idea}{}