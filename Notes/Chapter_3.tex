
\chapter{Sufficiency and Information}

\section{Sufficiency}

Consider a parametric model $\mcF = \{f(\cdot|\theta): \theta\in\Theta\}$ with $X_1,\ldots, X_n$ iid random sample of size $n$ extracted from $X$.
The goal is to use the information contained in $X_1,\ldots,X_n$ to produce inferential statements about the unknown parameter $\theta$.
When we compute statistics, i.e. functions of the random sample, we are summarizing the information contained in the random sample.
\opn{When can we be assured that in the process we are not losing any relevant information about the parameter?}{
    When all information that is lost is spurious, i.e. irrelevant for $\theta$.
}

\ex{Poisson}{
    $X | \lambda\sim Po(\lambda)$ where $\lambda > 0$. 
    We observe a random sample of size $n=2$ and we know that the observed value of the statistic $T=\sum_{i=1}^{n}X_i=31$.
    What can we say about the random sample $X_1, X_2$?\\

    We want the possibility of the sample given all the information that I have.
    \begin{align*}
        \frac{P(X_1=x_1, X_2=x_2|\lambda)}{P(T=31|\lambda)} & = \frac{\frac{e^{-\lambda}\lambda^{x_1}}{x_1!}\frac{e^{-\lambda}\lambda^{x_2}}{x_2!}}{\frac{e^{-2\lambda}(2\lambda)^{31}}{31!}}\\
        & =\frac{31!}{x_1!x_2!}\times\frac{\lambda^{x_1+x_2}}{\lambda^{31}}\times\left(\frac{1}{2}\right)^{31}\\
        & =\frac{31!}{(31-x_2)!x_2!}\times\left(\frac{1}{2}\right)^{x_2}\left(1-\frac{1}{2}\right)^{31-x_2}\\
        \Rightarrow\quad & X_2|T=31 \sim B(31,\frac{1}{2})
    \end{align*}
    We don't need to know $\lambda$.
}

\dfn[]{Sufficient statistics}{
    We say that a statistic $T$ is sufficient for $\mcF$ or for $\theta$, 
    if the conditional distribution of the random sample given the observed value of $T$ does not depend on the unkown parameter $\theta$ for all $\theta$.
}
What is essentially means is that $f(x_1,\ldots,x_n|\theta,t)$ does not depend on $\theta$.
\begin{align*}
    f(x_1,\ldots,x_n|\theta,t) & = \frac{f(x_1,\ldots,x_n,t|\theta)}{f_T(t,\theta)}\\
    & = \begin{cases*}
        \frac{f(x_1,\ldots,x_n|\theta)}{f_T(t,\theta)}\quad \text{if }T(x_1,\ldots,x_n)=t\\
        0\quad \text{otherwise}
    \end{cases*}
\end{align*}
\ex[]{Definition : Bernoulli}{
    $X|\theta\sim B(1,\theta)$, and $X_1,\ldots,X_n|\theta \overset{iid}{\sim} B(1,\theta)$\\

    Intuitively, the sequence of success and failure should not matter, only the proportion is important. 
    Thus, $T=\sum_{i=1}^{n}X_i$ should be sufficient for $\theta$.
    \begin{align*}
        f(x_1,\ldots,x_n|\theta,t) & = \frac{f(x_1,\ldots,x_n|\theta)}{f_T(t|\theta)} \quad\text{if }T(x_1,\ldots,x_n)=t, \sum x_i = t\\
        & =\frac{\prod_{i=1}^{n}f(x_i|\theta)}{f_T(t|\theta)}
    \end{align*}
    Because $T|\theta\sim B(n,\theta)$
    \begin{align*}
        f(x_1,\ldots,x_n|\theta,t) & = \frac{\prod_{i=1}^{n}\theta^{x_i}(1-\theta)^{1-x_i}}{\left(\begin{array}{c}n\\t\end{array}\right)\theta^t (1-\theta)^{n-t}}\quad \text{if }\sum x_i = t\\
        & =\frac{\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}}{\left(\begin{array}{c}n\\t\end{array}\right)\theta^t (1-\theta)^{n-t}}\\
        & = \frac{1}{\left(\begin{array}{c}n\\t\end{array}\right)}
    \end{align*}
}
The definition of sufficient statistic is not very useful to discover sufficient statistics.
Thus we need to rely on another theorem.
\thm[]{Halmos-Savage Factorization Criterion}{
    A statistic $T$ is sufficient for $\theta$ if and only if there are non-negative functions $g$ and $h$ such that
    \begin{itemize}
        \item $g$ depends on $\theta$ and on the random sample exclusively through the observed value of $T$.
        Meaning it depends on parameter and statistics.
        \item $h$ depends exclusively on the random sample.
        \item $f(x_1,\ldots,x_n|\theta) = g(T(x_1,\ldots,x_n);\theta)\times h(x_1,\ldots,x_n)$
    \end{itemize}
}
\begin{proof}
    $T$ is sufficient for $\theta \Leftrightarrow$ There are non-negative function of $g$ and $h$.\\

    We first start from left to right.\\

    If $T$ is sufficient for $\theta$, then
    \begin{equation*}
        f(x_1,\ldots,x_n|t,\theta) = \frac{f(x_1,\ldots,x_n|\theta)}{f_T(t|\theta)}\quad \text{does not depend on $\theta$}
    \end{equation*}
    meaning that
    \begin{equation*}
        f(x_1,\ldots,x_n|t) = \frac{f(x_1,\ldots,x_n|\theta)}{f_T(t|\theta)}\quad \text{if }T(x_1,\ldots,x_n)=t
    \end{equation*}
    so we get
    \begin{equation*}
        f(x_1,\ldots,x_n|\theta) = \underbrace{f_T(t|\theta)}_{g(t,\theta)}\underbrace{f(x_1,\ldots,x_n|t)}_{h(x_1,\ldots,x_n)}
    \end{equation*}

    Now going from right to left.\\

    For $X$ discrete, $T$ is discrete. And for simplification, $\tilde{x} = (x_1,\ldots,x_n)$.
    \begin{align*}
        f_T(t|\theta) & = P(T=t|\theta) = \sum_{\tilde{x}:T(\tilde{x})=t}f(\tilde{x}|\theta)\\
        & = \sum_{\tilde{x}:T(\tilde{x}=t)}g(t;\theta)h(\tilde{x})\\
        & = g(t;\theta)\sum_{\tilde{x}:T(\tilde{x})=t}h(\tilde{x})
    \end{align*}
    we now have
    \begin{align*}
        f(x_1,\ldots,x_n|t,\theta) & = \frac{f(x_1,\ldots,x_n|\theta)}{f_T(t|\theta)}\quad \text{if }T(\tilde{x})=t\\
        & = \frac{g(t,\theta)h(\tilde{x})}{g(t,\theta)\sum_{\tilde{x}:T(\tilde{x})=t}h(\tilde{x})}\\
        & = \frac{h(\tilde{x})}{\sum_{\tilde{x}:T(\tilde{x})=t}h(\tilde{x})} \quad\text{does not depend on $\theta$}
    \end{align*}
\end{proof}

\ex{Factorization criterion : Poisson}{
    Let $X|\lambda\sim Po(\lambda)$
    \begin{align*}
        f(x_1,\ldots,x_n|\lambda) & = \prod_{i=1}^{n}f(x_1|\lambda) = \prod_{i=1}^{n}e^{-\lambda}\frac{\lambda^{x_i}}{x_i!}\\
        & = e^{-n\lambda}\frac{\lambda^{\sum x_i}}{\prod x_i!}\\
        & = \underbrace{e^{-n\lambda}\lambda^{\sum x_i}}_{g(\sum x_i;\lambda)}\underbrace{\frac{1}{\prod x_i!}}_{h(\tilde{x})}
    \end{align*}
}
\ex[]{Factorization criterion : Uniform}{
    Let $X|\theta\sim U(0,\theta)$, $\theta > 0$.
    \begin{align*}
        f(x|\theta) & = \frac{1}{\theta}\quad ,0<x<\theta\\
        & = \frac{1}{\theta} \,I_{(0,\theta)}(x)
    \end{align*}
    with $I_A(x) = \begin{cases*}
        1\quad  ,x\in A\\
        0\quad  ,\text{otherwise}
    \end{cases*}$
    \begin{align*}
        f(\tilde{x}|\theta) & = \prod_{i=1}^{n}f(x_i|\theta) = \prod_{i=1}^{n}\frac{1}{\theta}\,I_{(0,\theta)}(x_i)\\
        & = \theta^{-n}\,I_{(0,\infty)}(x_{(1)})\,I_{(0,\theta)}(x_{(n)})\\
        & = \underbrace{\theta^{-n}I_{(0,\theta)}(x_{(n)})}_{g(x_{(n)})}\underbrace{I_{(0,\infty)}(x_{(1)})}_{h(\tilde{x})}
    \end{align*}
}
\ex[]{Factorization criterion : Shifted exponential}{
    Consider the shifted exponential distribution $f(x|\lambda,\delta) = \lambda e^{-\lambda(x-\delta)}\,I_{[\delta,\infty)}(x)$
    \begin{align*}
        f(\tilde{x}|\lambda,\delta) & = \prod_{i=1}^{n}f(x_i|\lambda,\delta) = \prod_{i=1}^{n} \lambda e^{-\lambda(x_i-\delta)}\,I_{[\delta,\infty)}(x_i)\\
        & = \underbrace{\lambda^n e^{-\lambda\sum x_i}e^{n\lambda\delta}\,I_{[\delta,\infty)}(x_{(1)})}_{g(\sum x_i, x_{(1)};\lambda,\delta)}\times \underbrace{1}_{h(\tilde{x})}
    \end{align*}
    By the factorization criterion, $(\sum x_i, x_{(1)})$ is sufficient for $(\lambda,\delta)$.
}
Two of things to notice regarding sufficient statistics.
\begin{itemize}
    \item There is always a sufficient statistic, it can be all the samples.
    \item Sufficient statistics are not unique.
\end{itemize}
\ex[]{Un-uniqueness of sufficient statistics : Poisson}{
    $X\sim Po(\lambda)$ where $t = \sum x_i$
    \begin{itemize}
        \item $f(\tilde{x}|\lambda) = e^{-n\lambda}\lambda^t\frac{1}{\prod x_i!} \Rightarrow T=\sum x_i$ is sufficient.
        \item $f(\tilde{x}|\lambda) = e^{-n\lambda}\lambda^{n\bar{x}}\frac{1}{\prod x_i!} \Rightarrow T=\bar{x}$ is sufficient.
        \item $f(\tilde{x}|\lambda) = e^{-n\lambda}\lambda^{T_1}\lambda^{T_2}\frac{1}{\prod x_i!} 
        \Rightarrow (T_1, T_2)$ is sufficient where $T_1 = \sum_{i=1}^{n_1}x_i$ and $T_2 = \sum_{i=n_1+1}^{n}x_i$.
    \end{itemize}
}

Another important concept is the \textbf{partition} induced in the sample space by a statistic.
\begin{itemize}
    \item Any statistics $T: \mcX \to \bbR^q$ induces a partition in the sample space $\mcX$ and the partitions do not intercept.
    \begin{center}
        \includegraphics[scale=0.5]{Images/5.png}
        $A + B + C + D = \mcX \Rightarrow T_1(\tilde{x}) = C \rightarrow \Pi = \{\mcX\}$
        \includegraphics[scale=0.5]{Images/6.png}
    \end{center}
    If a statistic takes 2 values : $T_1(\tilde{x}) = \begin{cases}
        c_1 &,\tilde{x}\in \mcX_1\\
        c_2 &,\tilde{x}\in \mcX_2
    \end{cases} \rightarrow \begin{cases}
        \mcX_1 \cup \mcX_2 = \mcX\\
        \mcX_1 \cap \mcX_2 = \varnothing\\
        \Pi = \{\mcX_1,\mcX_2\}
    \end{cases}$\\
    \includegraphics[scale=0.5]{Images/7.png}
    \item The finer the partition induced by $T$ in $\mcX$, the less information is lost; the smaller is the data reduction operated by $T$.
    \item A sufficient statistic operates a data reduction that does not involve loss of relevant information about the parameter; the partition it induces is also said to be sufficient.
\end{itemize}
\nt{
    When the partition is finer as it can be, 
    $T(\tilde{x}) = \tilde{x}$, $\displaystyle\Pi = \cup_{\tilde{x}\in\mcX}\{\tilde{x}\}$, 
    because no information will be lost.
}
\begin{itemize}
    \item The notion of partition is more general than that of statistic;
    different statistics can induce the same partition, in which case they are said to be equivalent, i.e. they are one-to-one.
    \item If the partition induced by $T$ is finer than the partition induced by $S$, then $S$ is a function of $T$.
    In that case, if $S$ is sufficient, so is $T$, that is\begin{equation*}
        \begin{rcases}
        S = h(T)\\
        S \:\text{sufficient}
    \end{rcases} \Rightarrow T \:\text{sufficient}
    \end{equation*}
    \begin{center}
        \includegraphics[scale=0.5]{Images/8.png} $S$ is a function of $T$
    \end{center}
    $\begin{cases}
        S \rightarrow \{\mcX_1,\mcX_2\}\\
        T \rightarrow \{\mcX_1, \mcX_{21}, \mcX_{22}\}
    \end{cases}$ If $S$ is sufficient, then $T$ is sufficient, because $T$ is finer than $S$ partition wise.
    \item However, if $T$ is sufficient and $S = h(T)$, it is not a given that $S$ is also sufficient unless $S$ is injective,
    in which case, $S$ and $T$ are equivalent.
    \item We are interested in finding statistics which are sufficient but operates the least data reduction.
    In other words, statistics that induce the coarsest partition that is still sufficient.
    \begin{center}
        \includegraphics[scale=0.5]{Images/9.png}
    \end{center}
    \textcolor{blue}{Point A} is what we want to find, the minimum sufficient statistic. 
    We lose information as we are moving to the right hand side. 
\end{itemize}

\dfn[]{Minimal sufficient statistic}{
    A statistic is said to be minimal sufficient for $\mcF$ if it is sufficient,
    and if $S$ is any other sufficient statistic, then $T = h(S)$ for some $h$.
}
Consider a binary relation in $\mcX$ defined by $\begin{cases}
    \tilde{y}\in\mcX\\
    \tilde{x}\in\mcX
\end{cases}$, $\tilde{y} \:\bmR\: \tilde{x}$.
\begin{equation*}
    f(\tdy|\theta) = c(\tdx, \tdy)f(\tdx|\theta)\Leftrightarrow\underbrace{\frac{f(\tdy|\theta)}{f(\tdx|\theta)}}_{\text{likelihood ratio}} = c(\tdx,\tdy)
\end{equation*}
where $c(\tdx,\tdy) > 0$ and does not depend on $\theta$.\\

This binary relation is an equivalence relation, that is, it is
\begin{itemize}
    \item symmetric: $\tdx\:\bmR\:\tdy \Leftrightarrow \tdy\:\bmR\:\tdx$
    \item reflexive: $\tdx\:\bmR\:\tdx$
    \item transitive: $\tdx\:\bmR\:\tdy$ and $\tdy\:\bmR\:\tdz\Rightarrow \tdx\:\bmR\:\tdz$
\end{itemize}
Hence, it induces a partition in $\mcX$ with parts
\begin{align*}
    \Pi_x & = \{y\in\mcX : y\:\bmR\:x\}\text{ for }x:f(x|\theta)>0 \text{ for some } \theta\in\Theta\\
    \Pi_0 & = \{y\in\mcX : f(y|\theta)=0\quad\forall\theta\in\Theta\}
\end{align*}
\begin{proof}
    Transitive\\

    Consider the case where
    \begin{align*}
        \tdx\:\bmR\:\tdy & \Rightarrow f(\tdy|\theta) = c_1f(\tdx|\theta)\\
        \tdy\:\bmR\:\tdz & \Rightarrow f(\tdy|\theta) = c_2f(\tdz|\theta)
    \end{align*}
    then we have
    \begin{equation*}
        c_1f(\tdx|\theta) = c_2f(\tdz|\theta) \Rightarrow f(\tdx|\theta) = \frac{c_2}{c_1}f(\tdz|\theta) \Rightarrow \tdx\:\bmR\:\tdz
    \end{equation*}
\end{proof}
\thm[]{Lehmann-Scheffe}{
    The partition with parts $\Pi_0$ and $\{\Pi_x\}$ described above is minimal sufficient, and any statistic which induces it is minimal sufficient. 
}
\begin{center}
    \includegraphics[scale=0.5]{Images/10.png} $\Pi = \{\Pi_1,\Pi_2,\Pi_3,\Pi_4\}$, $G(\tdx) = \tdx_i$ if $\tdx\in\Pi_i$
\end{center}
\begin{proof}
    Consider the statistic $G$ that to each $x\in\mcX$ associates a representative of the element of the partition to which it belongs
    \begin{equation*}
        x\in\mcX \mapsto x_{\Pi}\in\Pi_x = G(x)
    \end{equation*}
    \textbf{Proof that G is sufficient}
    \begin{gather*}
        \tdx\in\Pi_i \Rightarrow \tdx\:\bmR\:\tdx_i\\
        f(\tdx|\theta) = c(\tdx_i,\tdx)f(\tdx|\theta) \underset{G(\tdx)=\tdx_i}{=} \underbrace{c(\tdx_i,\tdx)}_{g(\tdx)}\underbrace{f(G(\tdx|\theta))}_{g(G;\theta)}\\
        \Rightarrow G \text{ is sufficient}
    \end{gather*}
    Now \textbf{proof that $G$ is a "grosser" partition}
    \begin{gather*}
        U \text{ is sufficient}\\
        \Pi_U = \{\Pi_x^*\},\quad
        \begin{rcases}
            \tdx\in\mcX\\
            \tdy\in\Pi_x^*
        \end{rcases}\rightarrow \tdx\in\Pi_i\\
        \text{since $U$ is sufficient,}\\
        f(\tdx|\theta) = g(U(\tdx);\theta)h(\tdx)\\
        f(\tdy|\theta) = g(U(\tdy);\theta)h(\tdy) = g(U(\tdx);\theta)h(\tdy)\Leftrightarrow\\
        h(\tdx)f(\tdy|\theta) = \underbrace{h(\tdx)g(U(\tdx);\theta)}_{f(\tdx|\theta)}h(\tdy)\Leftrightarrow\\
        f(\tdy|\theta) = f(\tdx|\theta)\frac{h(\tdy|\theta)}{h(\tdx|\theta)}\Rightarrow \tdy\:\bmR\:\tdx\Rightarrow U\text{ is a finer partition than }G\\
        \therefore G \text{ is the minimal sufficient statistic}
    \end{gather*}
\end{proof}

\ex[]{Minimal sufficient statistic : Normal}{
    Let $X_1,\ldots,X_n$ be a random sample from a $N(\mu,\sigma^2)$ population and let $\theta = (\mu,\sigma^2)$
    \begin{align*}
        f(\tdx|\theta) & = \prod_{i=1}^{n}f(x_i|\mu\sigma^2)\\
        & = \prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma^2}}\exp\{-\frac{1}{2\sigma^2}(x_i-\mu)^2\}\\
        & = (2\pi)^{-\frac{n}{2}}(\sigma^2)^{-\frac{n}{2}}\exp\{-\frac{1}{2\sigma^2}(\sum x_i^2-2n\bar{x}\mu+n\mu^2)\}\\
        & = (2\pi)^{-\frac{n}{2}}(\sigma^2)^{-\frac{n}{2}}\exp\{-\frac{1}{2\sigma^2}\sum x_i^2\}\exp\{\frac{n\bar{x}\mu}{\sigma^2}\}\exp\{-\frac{1}{2\sigma^2}n\mu^2\}
    \end{align*}
    Now using the \textbf{Lehmann-Scheffe theorem},
    \begin{align*}
        \frac{f(\tdy|\theta)}{f(\tdx|\theta)} 
        & = \frac{
            (2\pi)^{-\frac{n}{2}}(\sigma^2)^{-\frac{n}{2}}\exp\{-\frac{1}{2\sigma^2}\sum y_i^2\}\exp\{\frac{n\bar{x}\mu}{\sigma^2}\}\exp\{-\frac{1}{2\sigma^2}n\mu^2\}}{
                (2\pi)^{-\frac{n}{2}}(\sigma^2)^{-\frac{n}{2}}\exp\{-\frac{1}{2\sigma^2}\sum x_i^2\}\exp\{\frac{n\bar{x}\mu}{\sigma^2}\}\exp\{-\frac{1}{2\sigma^2}n\mu^2\}}\\
        & = \exp\{-\frac{1}{2\sigma^2}(\sum y_i^2-\sum x_i^2)\}\exp\{\frac{n\mu}{\sigma^2}(\bar{y}-\bar{x})\}\quad \text{does not depend on }\theta\\
        & \Leftrightarrow \begin{cases}
            \sum y_i^2 = \sum x_i^2\\
            \bar{y} = \bar{x}
        \end{cases}
    \end{align*}
    A minimal sufficient statistic is $(\bar{x},\sum x_i^2)$ or $(\bar{x}, s^2)$
}
\ex[]{Minimal sufficient statistic : Exponential}{
    Let $X_1,\ldots,X_n$ be a random sample from a $Ex(\lambda)$ population and let $\theta = \lambda$
    \begin{gather*}
        f(x|\lambda) = \lambda e^{-\lambda n}\, ,n>0\\
        f(\tdx|\lambda) = \prod_{i=1}^{n}\lambda e^{-\lambda n} = \lambda^n\exp\{-\lambda\sum x_i\}\\
        \frac{f(\tdy|\theta)}{f(\tdx|\theta)} = \frac{\lambda^n\exp\{-\lambda\sum y_i\}}{\lambda^n\exp\{-\lambda\sum x_i\}}
        = \exp\{-\lambda(\sum y_i-\sum x_i)\}
    \end{gather*}
    Does not depend on $\lambda$ if and only if $\sum y_i = \sum x_i \Leftrightarrow \bar{x}=\bar{y} \Rightarrow \bar{x}$ is minimal sufficient
}

\section{Ancilarity and completeness}
\dfn[]{Ancillary statistic}{
    A statistic $T$ is said to be ancillary if its sampling distribution does not depend on the unknown parameter $\theta$.
}
$T$ is ancillary if $f_T(t|\theta)$ does not depend on $\theta$.
\dfn[]{Location-scale family of distributions}{
    The location-scale family of distribution is composed by all the probability distributions such that
    the associated comulative distribution function is of the form
    \begin{equation*}
        G(x|\delta,\lambda) = G\left(\frac{x-\lambda}{\delta}\right)
    \end{equation*}
    where $G$ is a function that does not involve unknown parameters, $\lambda\in\bbR$ is the location parameter,
    and $\delta>0$ is the scale parameter.\\

    This family includes the location family ($\delta$ is known) and the scale family ($\lambda$ is known).
}

\ex[]{Location-scale family : Normal}{
    Suppose that $X\sim N(\mu, \sigma^2)$
    \begin{equation*}
        \frac{x-\mu}{\sigma} \sim N(0,1)
    \end{equation*}
    \begin{align*}
        f(x|\mu,\sigma^2)
        & = P(X\leq x | \mu\sigma^2)\\
        & = P\left(\frac{X-\mu}{\sigma}\leq\frac{x-\mu}{\sigma}|\mu,\sigma^2\right)\\
        & = \Phi\left(\frac{x-\mu}{\sigma}\right)
    \end{align*}
    where $G = \Phi, \lambda = \mu, \delta = \sigma$.\\

    The normal family of distributions is a member of the location-scale family with location parameter $\mu$ and scale parameter $\delta$.
}
\ex[]{Location-scale family : Uniform}{
    Suppose that $X\sim U(0,\theta)$, $\theta>0$, then we have
    \begin{equation*}
        f(x|\theta) = \frac{1}{\theta}\, I_{(0,\theta)}(x)
    \end{equation*}
    \begin{align*}
        F(x|\theta) 
        & = \frac{x}{\theta} \underbrace{I_{(0,\theta)}(x)}_{0<x<\theta\Leftrightarrow0<\frac{x}{\theta}<1}\\
        & = \frac{x}{\theta}\, I_{(0,1)}(\frac{x}{\theta})\\
        & = G\left(\frac{x}{\theta}\right)
    \end{align*}
    where $G(y) = y\,I_{(0,1)}(y)$\\

    $U(0,\theta)$ belongs to the scale family with scale parameter $\delta=\theta$ and $\lambda=0$.
}

Couple \textbf{remarks} regarding the location-scale family
\begin{itemize}
    \item The distribution of $X$ is part of the location-scale family with location parameter $\lambda$ and scale parameter $\delta$
    if and only if the distribution of $\frac{(x-\lambda)}{\delta}$ does not depend on unknown parameters.
    \item If the distribution of $X$ is a member of the location-scale family with location paratmer $\lambda$
    and scale parameter $\delta$ then any statistic which is a function of $X_1,\ldots,X_n$ only through the vector
    $\left(\frac{X_i-\lambda}{\delta}, i=1,\ldots,n\right)$ is ancillary.
    \begin{equation*}
        T = T(X_1,\ldots,X_n) = H\left(\frac{X_1-\lambda}{\delta},\ldots,\frac{X_n-\lambda}{\delta}\right),\quad T\text{ is ancillary}
    \end{equation*}
    TODO:
    \textcolor{red}{Insert proof}
\end{itemize}

\ex[]{Location-scale and statistic : Uniform}{
    Let $X|\theta \sim U(\theta - \frac{1}{2}, \theta + \frac{1}{2})$, $\theta\in\bbR$
    \begin{equation*}
        f(x|\theta) = \frac{1}{1}\underbrace{I_{(\theta-\frac{1}{2},\theta+\frac{1}{2})}(x)}_{\theta-\frac{1}{2}<x<\theta+\frac{1}{2}\Leftrightarrow -\frac{1}{2}<x-\theta<\frac{1}{2}} 
        = I_{(-\frac{1}{2},\frac{1}{2})}(x-\theta) = G(x-\theta)
    \end{equation*}
    where $G(y) = I_{(-\frac{1}{2},\frac{1}{2})}(y)$\\

    The distribution of $X$ belongs to the location family with $\lambda = \theta$ and $\delta = 1$. 
    The distribution of $x-\theta$ does not depend on unknwon parameters.
    \begin{equation*}
        R = X_{(n)}-X_{(1)} = (X_{(n)}-\theta)-(X_{(1)}-\theta) = H(X_1-\theta_1,\ldots,X_n-\theta_n)
    \end{equation*}
}
\begin{itemize}
    \item It would be natural to expect a minimal sufficient statistic and an ancillary statistic to be indepenendent.
    However, that is not the case in general.
\end{itemize}
\ex[]{Minimal sufficient statistic and ancillary statistic : Uniform}{
    Let $X|\theta \sim U(\theta - \frac{1}{2}, \theta + \frac{1}{2})$
    \begin{equation*}
        \frac{f(\tdy|\theta)}{f(\tdx|\theta)} = \frac{I_{(y_{(n)}-\frac{1}{2},\infty)}(\theta)I_{(-\infty,y_{(1)}+\frac{1}{2})}(\theta)}{I_{(x_{(n)}-\frac{1}{2},\infty)}(\theta)I_{(-\infty,x_{(1)}+\frac{1}{2})}(\theta)}
    \end{equation*}
    does not depend on $\theta$ if and only if $\begin{cases}
        x_{(n)} = y_{(n)}\\
        x_{(1)} = y_{(1)}
    \end{cases} \Rightarrow (x_{(1)}, x_{(n)})$ is minimal sufficient, $(x_{(n)}-x_{(1)}, x_{(1)})$ is minimal sufficient.\\

    $(R, X_{(1)})$ is minimal sufficient but contains an element that is ancillary.
}
\dfn[]{Complete statistic}{
    A statistic $T$ is said to be complete if and only if 
    \begin{equation*}
        E[h(T)|\theta]=0\; \forall \theta\in\Theta \Rightarrow h(T)\equiv 0
    \end{equation*}
}
When $T$ is complete and $h_1(T)$ and $h_2(T)$ are two functions of $T$ that have the same expected value,
then it must be the case that they are the same, $h_1(T) = h_2(T)$.
\begin{proof}
    \begin{flalign*}
        & E_{\theta}[h_1(T)-h_2(T)] = 0\quad \forall\theta\in\Theta\\
        & \Downarrow\\
        & h_1(T)-h_2(T) = 0\\
        &\Updownarrow\\
        & h_1(T) = h_2(T)\quad \text{must be the same}
    \end{flalign*}
\end{proof}

\thm[]{}{Any statistic that is sufficient and complete is minimal sufficient.}
\begin{proof}
    Assume $T$ is sufficient and complete.
    \begin{flalign*}
        & \exists T_1\quad T_1=g(T),\quad T_1\text{ sufficient}\\
        & h(T) = T - \underbrace{E[T|T_1]}_{\text{function of $T_1$ because }T_1=g(T)}
    \end{flalign*}
    Conditional expectation: $E[X|Y=y] = \int_X x f_{X|Y=y}(x)\; dx$ which is a function of $y$.
    This expectation could be a function of the parameter, need to make sure that it's not the case for $h(T)$ to be a statistic.
    \begin{equation*}
        E[T|T_1=t_1] = \int_x T(\tdx)f_{\tdx|T_1=t_1,\theta}(\tdx)\;d\tdx
    \end{equation*}
    Since $T_1$ is sufficient, by definition, $f_{\tdx|T_1=t_1,\theta}(\tdx)$ does not depend on $\theta\Rightarrow E[T|T_1]$ does not depend on $\theta$.
    $\Rightarrow h(T)$ is a statistic.
    \begin{align*}
        E_{\theta}[h(T)] & = E_{\theta}[T]-\underbrace{E_{\theta}[E[T|T_1]]}_{\text{law of iterated expectation}}\\
        & = E_{\theta}[T] - E_{\theta}[T] = 0
    \end{align*}
    Since $T$ is complete, $h(T) = 0\Leftrightarrow T=E[T|T_1] = f(T_1)$
\end{proof}
\wc[]{Converse}{
    Let $X|\theta\sim U(\theta-\frac{1}{2},\theta+\frac{1}{2})$\\

    $R = X_{(n)} - X_{(1)}$ which is ancillary $\to$ distribution does not depend on $\theta$\\

    $T(R, X_{(1)})$ is minimal sufficient but $R$ has no information about $\theta$ so should throw away,
    but if you throw away then it is not minimal sufficient.
    \begin{equation*}
        h(T) = R - \underbrace{E[R]}_{= c\text{ because $R$ is ancillary}}
    \end{equation*}
    $E[h(T)] = 0$ but $h(T)\neq 0\Rightarrow T$ is not complete. 
}
Turns out that sufficient and complete statistics operate a data reduction that is more effective than that operated by minimal sufficient statistics that are not complete.
This is the subject of the Theorem of Basu.
\thm[]{Basu's Theorem}{
    Let $T$ be a sufficient and complete statistic. Then $T$ is independent of any ancillary statistic.
}
\begin{proof}
    Assume\begin{itemize}
        \item $T$ is sufficient and complete
        \item $U$ is ancillary
        \item $P(U\in A|T) = h_A(T)$ because conditioning on T
    \end{itemize}
    $h_A(T)$ is a statistic and $T$ is sufficient so does not depend on $\theta$.
    \begin{align*}
        P(U\in A|T=t) & = \int_x 1_A(U(\tdx))f_{\tdx|T=t,\theta}(\tdx)\, d\tdx\\
        & = \int_{\{\tdx:U(\tdx)\in A\}}U(\tdx)f_{\tdx|T=t,\theta}(\tdx)\,d\tdx\quad \text{where $T=t,\theta$ does not depend on $\theta$ because $T$ sufficient}
    \end{align*}
    but $E[h_A(T)]$ could depend on $\theta$ but does not because 
    \begin{flalign*}
        & h_A(T) = P(U\in A|T) = E[1_A(U)|T]\to \text{Bernoulli}\\
        & E[h_A(T)] = E[E[1_A(U)|T]] = E[1_A(U)] = \underbrace{P(U\in A)}_{\text{marginal probability}} = c\quad \text{does not depend on $\theta$, so a constant}\\
        & h_A(T) - c\quad \text{still a statistic because (statistic - constant) = statistic}\\
        & E[h_A(T) - c] = 0 \overset{T \text{ complete}}{\Longrightarrow} h_A(T) = c \overset{A \text{ arbitrary}}{\Longleftrightarrow} P(U\in A|T) = P(U\in A)
    \end{flalign*}
    $\Rightarrow$ $U$ and $T$ are independent.
\end{proof}

%\dfn{Definition Topic}{Definition Statement}
%\thm{Theorem Name}{Theorem Statement}
%\cor[cori]{Corollary Name}{Corollary Statement}
%\lem{Lemma Name}{Lemma Statement}
%\clm{Claim Name}{Claim Statement}
%\ex{Example Name}{Example explained}
%\opn{Open Question Name}{Question Statement}
%\pr{Question Name}{Question Statement}
%\nt{Special Note}
%\wc{Wrong Concept topic}{Explanation}
%\proof{Proof Idea}{}