\chapter{Parametric Point Estimation}

The context of the problem:
\begin{itemize}
    \item Parametric statistical model for $\bar{X}, \mcF=\{f(\cdot|\theta): \theta\in\Theta\}$
    \item $X_1,\ldots,X_n$ are iid random sample of size $n$ extracted from $X$
    \item Samples space is denoted by $\mcX$
\end{itemize}
The problem itself:
\begin{itemize}
    \item To produce a \textbf{point estimate of $\theta$}, that is, select an application $x\in\mcX\mapsto T(x)\in\Theta$
    that to each observed sample associates a value for $\theta$
    \item The application $T$ is a statistic which we call an \textbf{estimator} for $\theta$ and the observed value of $T(x)$ is called the \textbf{estimate}
    \item We may be interested in estimating a function of $\theta, \tau(\theta)$
\end{itemize}

\section{Optimality criteria}

In frequentist statistics, the quality of an estimator is assessed by looking at the population of estimates it produces, that is, at its sampling distribution.
We are \textbf{evalauting the estimator} and not the estimate.
\nt{
    $E[\bar{X}] = \lambda\to$ pre-experimental.\\
    $\bar{X}\to$ post-experimental, not so intersted in this accuracy.
}

\subsection{Unbiasedness}

\dfn[]{Unbiased estimator}{
    An estimator $T$ is said to be an unbiased estimator of $\tau(\theta)$ if and only if 
    \begin{equation*}
        \forall \theta\in\Theta\quad E_{\theta}[T] = \tau(\theta)
    \end{equation*}
}

In practice, this means that if we use $T$ to estimate $\tau(\theta)$ a very large number of times,
then the average of the estimates will be close to $\tau(\theta)$ no matter the true value of $\theta$.
\begin{proof}
    $E[T(\tdx)] = \tau(\theta)$
    \begin{equation*}
        \begin{cases}
            \tdx_1\to T(\tdx_1)\\
            \vdots\\
            \tdx_N\to T(\tdx_N)
        \end{cases}\Rightarrow \frac{1}{N}\sum_{i=1}^{N}T(\tdx_i)\overset{LLN}{=}\tau(\theta)
    \end{equation*}
\end{proof}

The quantity $b(T) = E[T|\theta] - \tau(\theta)$ is known as the \textbf{bias of the estimator of $\tau(\theta)$}.
An estimator which is not unbiased is said to be biased.
There are cases where an unbiased estimator cannot be found.
In general, the sample mean is an unbiased estimator of the population mean as long as it exists.
The bias-corrected variance is an unbiased estimator of the population variance as long as it exists.
By restricting the class of interesting estimators to the class of unbiased estimators, we may miss intersting estimators.

\subsection{Most efficient estimation}

\dfn[]{More efficient estimator}{
    Let $T$ and $T^*$ be two \textbf{unbiased} estimators of $\tau(\theta)$. We say that $T$ is more efficient than $T^*$ in the estimation of $\tau(\theta)$ if 
    \begin{equation*}
        Var_{\theta}(T)\leq Var_{\theta}(T^*),\quad \forall \theta\in\Theta
    \end{equation*}
}

\thm[]{Cramer-Rao inequality}{
    Consider a statistical model satisfying the \textbf{regularity conditions} with $\Theta\subset\bbR$,
    and let $\tau(\theta)$ be a differentiable function. Let $T$ be an \textbf{unbiased estimator} of $\tau(\theta)$ with finite variance.
    Additionally, assume that $\forall\theta\in\Theta$
    \begin{flalign*}
        & E_{\theta}[T(X-1,\ldots,X_n)S(\theta|X_1,\ldots,X_n)] < +\infty\\
        & E_{\theta}[T(X-1,\ldots,X_n)S(\theta|X_1,\ldots,X_n)] = \tau'(\theta) \to (1)
    \end{flalign*}
    in which case we say that $T$ is a \textbf{regular estimator}. Then,
    \begin{equation*}
        Var_{\theta}(T) \geq \frac{[\tau'(\theta)]^2}{nI_{X}(\theta)}\to\text{ Cramer-Rao lower bound (CRLB)}
    \end{equation*}
}
\begin{proof}
    $(1): E_{\theta}[T(X-1,\ldots,X_n)S(\theta|X_1,\ldots,X_n)] = \tau'(\theta)$
    \begin{align*}
        E[TS] & = \int_x T(\tdx)\frac{\partial \ln f(\tdx|\theta)}{\partial\theta}f(\tdx|\theta)\,d\tdx 
        = \int_x T(\tdx)\frac{\frac{\partial f(\tdx|\theta)}{\partial\theta}}{f(\tdx|\theta)}f(\tdx|\theta)\,d\tdx\\
        & = \int_x \frac{\partial T(\tdx)f(\tdx|\theta)}{\partial\theta}\,d\tdx 
        = \frac{\partial}{\partial\theta}\int_x T(\tdx)f(\tdx|\theta)\,d\tdx 
        = \frac{\partial}{\partial\theta}E[T] = \tau'(\theta)
    \end{align*}
\end{proof}
\begin{proof}
    Cramer-Rao inequality
    \begin{equation*}
        Cov(T,S) = E[TS] - E[T]\underbrace{E[S]}_{=0, \text{ regularity condition}} = E[TS] = \tau'(\theta)
    \end{equation*}
    Using Cauchy-Swartz inequality,
    \begin{flalign*}
        & [\rho(T,S)]^2\leq 1\Leftrightarrow\\
        & \left[\frac{Cov(T,S)}{\sqrt{Var(T)Var(S)}}\right]^2\leq 1\Leftrightarrow \frac{[\tau'(\theta)]^2}{Var(T)Var(S)}\leq 1\Leftrightarrow\\
        & Var(T) \geq \frac{[\tau'(\theta)]^2}{\underbrace{Var(S)}_{= I_{\tdx}(\theta)\text{ regularity condition}}} = \frac{[\tau'(\theta)]^2}{I_{\tdx}(\theta)}
    \end{flalign*}
\end{proof}

The Cramer-Rao lower bound is only meaningful under the regularity conditions. 
Even when then regularity conditions are satisfied, there might not exist an estimator whose variance equals the CRLB.

\dfn[]{Efficiency}{
    The ratio between the CRLB and the variance of an \textbf{unbiased estimator} of $\tau(\theta)$ is known as efficiency 
    \begin{equation*}
        e(T) = \frac{\text{CRLB}}{Var_{\theta}(T)}
    \end{equation*}
}
If the regularity conditions are satisfied, $0\leq e(T) \leq 1$. And if $T$ is an \textbf{unbiased} estimator of $\tau(\theta)$ and $e(T) = 1$, 
then $T$ is known as the \textbf{most efficient estimator} of $\tau(\theta)$.
There is also the notion of \textbf{asymptotic efficiency} $\lim_{n\to\infty}e(T)$ and also of asymptotically most efficient estimators.
\ex[]{Most efficient estimator : Poisson}{
    $X_1, \ldots, X_n\sim Po(\lambda)$
    \begin{flalign*}
        & \tau(\theta) = \lambda\Rightarrow\tau'(\theta) = 1\\
        & Var(T) \geq \frac{1}{I_x(\lambda)} = \frac{\lambda}{n}\\
        & Var(\bar{X}) = \frac{\lambda}{n} = \text{CRLB}\Rightarrow \bar{X}\text{ is the most efficient estimator of }\lambda
    \end{flalign*}
    Another case,
    \begin{flalign*}
        & \tau(\theta) = e^{-\lambda}\Rightarrow \tau'(\theta) = -e^{-\lambda}\\
        & Var(W) \geq \frac{e^{-2\lambda}}{n/\lambda} = \frac{\lambda e^{-2\lambda}}{n}\\
        & \Rightarrow \text{If I find the estimator with this variance, then I've found the most efficient estimator}
    \end{flalign*}
}

\cor[]{CR theorem: Existance of most efficient estimator}{
    Let $T$ be a \textbf{regular and unbiased} estimator of $\tau(\theta)$.
    Then $T$ is the most efficient estimator of $\tau(\theta)$ if and only if there exists $a(\theta)$ such that 
    \begin{equation*}
        S(\theta|x_1,\ldots,x_n) = a(\theta)[T(x_1,\ldots,x_n)-\tau(\theta)]
    \end{equation*}
}
\begin{proof}
    Begin with the condition of the Cauchy-Swartz inequality 
    \begin{equation*}
        T = a + bS \Leftrightarrow S = a + bT
    \end{equation*}
    Since $E[S] = 0$,
    \begin{equation*}
        0 = a + b\tau(\theta) \Leftrightarrow a = -b\tau(\theta)\Rightarrow S = -b\tau(\theta) + bT \Leftrightarrow S = b(T - \tau(\theta))
    \end{equation*}
\end{proof}

\cor[]{Sufficient \& 1-parameter exponential family}{
    The CRLB in the estimation of $\tau(\theta)$ is attained by an estimator $T$ if and only if 
    $T$ is a \textbf{sufficient} statistic in the \textbf{one-parameter exponential family} with density 
    \begin{equation*}
        f(x) = h(x) c(\theta) \exp[Q(\theta)T(x)]
    \end{equation*}
    where $c(\theta) = \int a(\theta)\tau(\theta)\, d\theta$ and $Q(\theta) = \int a(\theta)\,d\theta$.
}
\begin{proof}
    \begin{flalign*}
        & S(\theta|\tdx) = \frac{\partial \ln f(\tdx|\theta)}{\partial\theta} = a(\theta)[T-\tau(\theta)]\\
        & \Rightarrow \ln f(\tdx|\theta) = A(\theta)T - B(\theta)+c(\tdx)\\
        & \Rightarrow f(\tdx|\theta) = \underbrace{e^{c(\tdx)}}_{h(\tdx)}\overbrace{\underbrace{e^{-B(\theta)}}_{c(\theta)}\exp\{\underbrace{A(\theta)}_{\omega_1(\theta)}\underbrace{T(\tdx)}_{R_1(\tdx)}\}}^{g(T;\theta)}
    \end{flalign*}
\end{proof}

If there is a most efficient estimator for $\tau(\theta)$ then $T$ must be suffcient for $\theta$.
There aren't most efficient estimators in models that do not admist one-dimentional sufficient statistics.

\ex[]{Most efficient estimator : Bernoulli}{
    $X\sim B(1,\theta)$
    \begin{flalign*}
        & f(\tdx|\theta) = \prod_{i=1}^{n}\theta^{x_i}(1-\theta)^{1-x_i} = \theta^{\sum x_i}(1-\theta)^{n-\sum x_i}\\
        & \ln f(\tdx|\theta) = \sum x_i\ln\theta + (n-\sum x_i)\ln(1-\theta)\\
        & S(\theta|\tdx) = \frac{\partial\ln f(\tdx|\theta)}{\partial\theta} = \frac{\sum x_i}{\theta} + (n-\sum x_i)\frac{-1}{1-\theta} = \cdots 
        = \frac{1}{\theta(1-\theta)}(\sum x_i-n\theta) = \frac{n}{\theta(1-\theta)}(\bar{X}-\theta)\\
        & \Rightarrow \bar{X} \text{ is the most efficient estimator of }\theta
    \end{flalign*}
    \begin{flalign*}
        & E[\bar{X}] = \theta = \tau(\theta)\\
        & Var(\bar{X}) = \frac{[\tau'(\theta)]^2}{nI_X(\theta)} = \frac{1}{nI_X(\theta)}\\
        & \text{since }Var(\bar{X}) = \frac{\theta(1-\theta)}{n} = \frac{1}{nI_X(\theta)} \Rightarrow nI_X(\theta) = \frac{1}{\theta(1-\theta)}
    \end{flalign*}
}
\ex[]{Most efficient estimator : Exponential}{
    $X\sim Ex(\lambda)$
    \begin{flalign*}
        & f(x|\lambda) = \lambda e^{-\lambda x},\quad x>0\\
        & f(\tdx|\theta) = \prod_{i=1}^{n}\lambda e^{-\lambda x_i} = \lambda^n e^{-\lambda\sum x_i}\\
        & \ln f(\tdx|\lambda) = n\ln\lambda - \lambda\sum x_i\\
        & S(\lambda|\tdx) = \frac{\partial f(\tdx|\lambda)}{\partial\lambda} = \frac{n}{\lambda} - \sum x_i 
        = \underbrace{-n}_{a(\lambda)}(\underbrace{\bar{X}}_{T} - \underbrace{\frac{1}{\lambda}}_{\tau(\lambda)})
    \end{flalign*}
    $\bar{X}$ is the most efficient estimator of $\tau(\lambda) = \frac{1}{\lambda}\to \tau'(\lambda) = -\frac{1}{\lambda^2}$
    \begin{flalign*}
        & E[\bar{X}] = \frac{1}{\lambda} = \tau(\lambda)\\
        & Var(\bar{X}) = \frac{[\tau'(\theta)]^2}{nI_X(\lambda)} = \frac{\frac{1}{\lambda^4}}{nI_X(\lambda)}\Leftrightarrow\\
        & I_X(\lambda) = \frac{1}{\lambda^2}
    \end{flalign*}
}

Once we determine that $T$ is the most efficient estimator of $\tau(\theta)\Rightarrow E[T] = \tau(\theta); Var(T) = \frac{[\tau'(\theta)]^2}{nI_X(\theta)}$
which may allow us to determine $I_X(\lambda)$ if we know $Var(T)$ or vice versa.

\subsection{Uniformly minimum-variance unbiased estimation}

\dfn[]{Uniformly minimum-variance unbiased estimator (UMVUE)}{
    Let $T$ be an \textbf{unbiased} estimator of $\tau(\theta)$. If for any other unbiased estimator of $\tau(\theta)$, $W$, we have
    \begin{equation*}
        Var(T|\theta) \leq Var(W|\theta),\quad \forall\theta\in\Theta
    \end{equation*}
    then $T$ is the so called uniformly minimum-variance unbiased estimator of $\tau(\theta)$, or UMVUE.
}

\thm[]{Rao-Blackwell}{
    Let $T$ be a \textbf{sufficient} statistic for $\theta$ and $U$ an \textbf{unbiased} estimator of $\tau(\theta)$. 
    Then $E[U|T]$ is an \textbf{unbiased} estimator of $\tau(\theta)$ with variance that is never superior to that of $U$.
    The two variances coincide if and only if $U$ is a function of $T$.
}
\nt{The process of computing $E[U|T]$ is called Rao-Blackwellization.}

\begin{proof}
    $U$ unbiased estimator of $\tau(\theta)$, $T$ sufficient statistic.
    \begin{flalign*}
        & E[U|T] \text{ is a statistic}\\
        & E[U|T=t] = \int_x U(\tdx)\underbrace{f(\tdx|t)}_{\text{doesn't depend on $\theta$ because $T$ sufficient}}dx\\
        & E[E[U|T]] = E[U] = \tau(\theta) \Rightarrow E[U|T]\text{ unbiased estimator of }\tau(\theta)\\
        & Var(U) = \underbrace{E[Var(U|T)]}_{\geq 0}+Var(E[U|T])\\
        & Var(U)\geq Var(E[U|T])\\
        & Var(U) = Var(E[U|T]) \Leftrightarrow E[Var(U|T)] = 0 \Leftrightarrow U\text{ is a function of }T
    \end{flalign*}
\end{proof}

\ex[]{Rao-Blackwell : Normal}{
    $X_1,\ldots,X_n\sim n(\mu,1),\quad \tau(\theta) = \mu^2+1$\\

    We know that $U = \frac{\sum X_i^2}{n} = S^2+\bar{X}^2$ is an unbaised estimator of $\tau(\mu)$
    \begin{flalign*}
        & E[U] = \mu^2 + 1\\
        & \bar{X} \text{ is sufficient for }\mu\\
        & E[U|\bar{X}] = E[S^2+\bar{X}^2|\bar{X}] = E[S^2|\bar{X}] + E[\bar{X}^2|\bar{X}]\\
        & \bar{X}\ind S^2 \Rightarrow E[S^2|\bar{X}] = E[S^2] = \frac{n-1}{n}\times 1\\
        & \text{Also, }E[\bar{X}^2|\bar{X}] = \bar{X}^2\\
        & \Rightarrow E[U|\bar{X}] = \frac{n-1}{n}+\bar{X}^2
    \end{flalign*}
}

The UMVUE should be a \textbf{function of a sufficient statistic}. 
When we Rao-Blackwellize an unbiased estimator, we do not obtain necessarily the UMVUE because there is always the possibility that if we had started with another unbaised estimator
we could haveobtained a smaller variance.
That cannot happen if we \textbf{start with a sufficient and complete statistic}:
Let $g(T) = E[U|T]$ and $g^*(T)$ another unbiased estimator which is also a function of $T$. The \textbf{completeness} of $T$ implies that $g(T) = g^*(T)$.

\thm[]{Lehmann-Scheffé}{
    If the statistical model admits a \textbf{sufficient and complete} statistic $T$ and there is at least an \textbf{unbiased} estimator of $\tau(\theta)$
    then there is an UMVUE for $\tau(\theta)$ that is unique and a function of $T$.\\

    Startegies to find UMVUEs in models admitting sufficient and complete statistic:
    \begin{enumerate}
        \item Obtain an unbiased estimator and Rao-Blackwellize it using the sufficient and complete statistic.
        \begin{itemize}
            \item Find $U: E[U] = \tau(\theta)$
            \item Find $T$ sufficient and complete $\to$ compute $E[U|T]$
        \end{itemize}
        \item Directly identify an unbiased estimator that is a function of a complete and sufficient statistic.
        \begin{itemize}
            \item Find $T$ sufficient and complete
            \item Find $g(T)$ such that $E[g(T)] = \tau(\theta)$
        \end{itemize}
    \end{enumerate}
}

\ex[]{Lehmann-Scheffé : Poisson}{
    Conditions:
    \begin{itemize}
        \item $X_1\ldots,X_n|\theta \sim Po(\theta)$
        \item $Po(\theta)$ is a member of the 1-parameter exponential family with $\alpha = \ln\theta\in\bbR$
        \item $T = \sum x_i$ is sufficient and complete
    \end{itemize}
    Using method 1:
    \begin{align*}
        U = I_{\{0\}}(X_1) & \Rightarrow E[U] = P(X_1 = 0) = e^{-\theta}\\
        E[U|T=t] & = E[I_{\{0\}}(X_1)|T=t] = P(X_1 = 0|T=t) = \frac{P(X-1=0; \sum_{i=2}^{n}X_1=t)}{P(T=t)} = \frac{P(X_1=0)P(\sum_{i=2}^{n}X_1=t)}{P(T=t)}\\
        & \left[\begin{cases}
            \sum_{i=2}^{n}x_i\sim Po((n-1)\theta)\\
            T\sim Po(n\theta)
        \end{cases}\right] \\
        & = \frac{e^{-\theta}\frac{e^{-(n-1)\theta}[(n-1)\theta]^t}{t!}}{e^{-n\theta}\frac{(n\theta)^t}{t!}}=\left(\frac{n-1}{n}\right)^t \rightarrow \text{ UMVUE}
    \end{align*}
    Using method 2:
    \begin{flalign*}
        & E[g(T)] = \sum_{t=0}^{\infty} g(T) e^{-n\theta}\frac{(n\theta)^t}{t!} = e^{\theta},\quad \forall \theta>0\Leftrightarrow\\
        & \sum_{t=0}^{\infty} g(T)\frac{(n\theta)^t}{t!} = e^{n\theta}e^{-\theta},\quad \forall \theta>0\Leftrightarrow\\
        & \sum_{t=0}^{\infty} g(T) n^t \frac{\theta^t}{t!} = e^{(n-1)\theta},\quad \forall \theta>0\Leftrightarrow (1)\\
        & \sum_{t=0}^{\infty} g(T) n^t \frac{\theta^t}{t!} = \sum_{t=0}^{\infty}(n-1)^t\frac{\theta^t}{t!},\quad \forall \theta>0 \Rightarrow\\
        & g(t)n^t = (n-1)^t \Leftrightarrow g(t) = \left(\frac{n-1}{n}\right)^t\to \text{ UMVUE}
    \end{flalign*}
    (1):
    \begin{flalign*}
        & e^x = \sum_{j=0}^{\infty}\frac{x^j}{j!}\\
        & e^{(n-1)\theta} = \sum_{t=0}^{\infty}\frac{[(n-1)\theta]^t}{t!} = \sum_{t=0}^{\infty}(n-1)^t\frac{\theta^t}{t!}
    \end{flalign*}
}

Relating efficiency and UMVU. If the model is \textbf{regular} and there exists a \textbf{most efficient estimator} for $\tau(\theta)$, then this estimator is necessarily the \textbf{UMVUE}.
The variance of the UMVUE will not necessarily be equal to the CRLB.
If the model is regular, there may not exist most efficient estimators, in this case the UMVUE will have a variance strictly larger than the CRLB.
The \textbf{UMVU criterion does not depend on regularity conditions}, and hence, if theses are not met, the variance of the UMVUE may be smaller than the CRLB defined as if the model were regular.

\subsection{Mean square error}
This is used to compare unbiased estimators in terms of the \textbf{dispersion} of their distribution around $\tau(\theta)$.
\dfn[]{Mean square error}{
    The mean square error of an estimator $T$ of $\tau(\theta)$ is 
    \begin{equation*}
        MSE(T) = E_{\theta}[(T-\tau(\theta))^2]
    \end{equation*}
}

Couple \textbf{remarks}:
\begin{itemize}
    \item $T$ will be superior to $T^*$ in mean squared error in the estimation of $\tau(\theta)$ if $MSE(T\leq MSE(T^*)),\;\forall\theta$.
    \item Markov inequality indicates that $P(|T-\tau(\theta)|>\epsilon)\leq \frac{MSE(T)}{\epsilon^2}$
    \item We have $MSE(T) = VAR_{\theta}(T) + [b(T)]^2$
    \begin{proof}
        \begin{align*}
            E_{\theta}[(T-\tau(\theta))^2] & = E[(T-\mu+\mu-\tau(\theta))^2] = E[(T_\mu)^2]\\
            & = E[(T-\mu)^2+2(\mu-\tau(\theta))(T-\mu)+(\mu-\tau(\theta))^2]\\
            & = Var(T) + 0 + (\mu+\tau(\theta))^2 = Var(T)+b_T^2
        \end{align*}
    \end{proof}
\end{itemize}

\ex[]{MSE : Normal}{
    $X\sim N(\mu,\sigma^2)$
    \begin{flalign*}
        & E[S'^2] = \sigma^2\\
        & E[S^2] = \frac{n-1}{n}\sigma^2\\
        & \frac{2\sigma^2}{n} = Var(S'^2) > MSE(S^2) = \frac{2\sigma^2}{n}-\frac{3\sigma^2}{n^2}
    \end{flalign*}
    $\Rightarrow$ According to the MSE criterion, $S^2$ is better than $S'^2$.
}

\subsection{Consistency}

Consistency looks at the behavior of the sampling distribution of an estimator as $n\to +\infty$.
\dfn[]{Consistency}{
    Let $T_n = T(X_1,\ldots, X_n)$. The estimator $T_n$ is said to be a (weakly) consistent estimator of $\tau(\theta)$ if, for all $\theta\in\Theta$,
    \begin{equation*}
        \forall \epsilon>0\quad \lim_{n\to+\infty}P(|T_n-\tau(\theta)|>\epsilon) = 0
    \end{equation*}
    that is, if, for all $\theta\in\Theta, T_n\xrightarrow{P}\tau(\theta)$.
}
\dfn[]{Mean-square consistency}{
    The estimator $T_n$ is said to be a mean-square consistent estimator of $\tau(\theta)$ if, for all $\theta\in\Theta$,
    \begin{equation*}
        \lim_{n\to+\infty}E_{\theta}[(T_n-\tau(\theta))^2] = 0
    \end{equation*}
    that is, if, for all $\theta\in\Theta, T_n\xrightarrow{m.s.}\tau(\theta)$.
}

Couple \textbf{remarks}:
\begin{itemize}
    \item From the Markov inequality, we know that mean-square convergence implies convergence in probability, so mean-square consistency implies weak consistency.
    \item Since $E_{\theta}[(T_n-\tau(\theta))^2] = MSE(T_n) = Var(T_n) + [b(T)]^2$, a \textbf{sufficient} condition for weak sonsistency of $T_n$ is that 
    \begin{gather*}
        \lim_{n\to+\infty}E[T_n] = \tau(\theta)\\
        \text{and}\\
        \lim_{n\to+\infty}Var[T_n] = 0
    \end{gather*}
    \item Consistency is not a very restrictive property. It is useful to exclude estimators though.
\end{itemize}

\section{Estimation Methods}

\subsection{Method of moments}

The idea is to \textbf{estimate population moments by their corresponding sample moments}.
Let $\theta = (\theta_1,\ldots,\theta_k)$ be a vector of unknown parameters of the population, $\mu' = E[X^r]$ will necessarily be a function of $\theta: \mu_r' = \psi_r(\theta)$.
Considering the corresponding sample moments $M'_r = \sum_{i=1}^{n}\frac{X_i^r}{n}$ and form the system of equations $M'_r = \psi_r(\theta)$.
The solution to this equation determines a methods of moments estimator of $\theta: (\phi_r(X_1,\ldots,X_n), r = 1,\ldots,k)$.

\nt{Recall the close relationship between the sample moments and the raw moemtns in the asymptotic sense.}

\ex[]{A method of moment : Normal}{
    $X_1,\ldots,X_n\sim N(\mu,\sigma^2)$
    \begin{flalign*}
        & \theta = (\theta_1,\theta_2) = (\mu, \sigma^2)\\
        & \begin{cases}
            \mu'_1 = \mu = \theta_1 \\ \sigma^2 = \mu'_2 - (\mu'_1)^2
        \end{cases} \Leftrightarrow \begin{cases}
            \mu'_1 = \mu = \theta_1 \\ \mu'_2 = \sigma^2 + \mu^2 = \theta_1 + (\theta_2)^2
        \end{cases}\\
        & \begin{cases}
            M'_1 = \mu \\ M'_2 = \sigma^2 - \mu^2
        \end{cases} \Leftrightarrow \begin{cases}
            \mu = M'_1 \\ \sigma^2 = M'_2 - (M'_1)^2 = S^2
        \end{cases} \Rightarrow \begin{cases}
            \tilde{\mu} = \bar{X} \\ \tilde{\sigma}^2 = S^2
        \end{cases}
    \end{flalign*}
    This is a method of moment, can also start with central moments as well.
}
\ex[]{A method of moment : Gamma}{
    $X_1,\ldots,X_n\sim G(\alpha,\lambda)$
    \begin{flalign*}
        & \begin{cases}
            \mu'_1 = \frac{\alpha}{\lambda} \\ \mu'_2  - (\mu'_1)^2= \frac{\alpha}{\lambda^2}
        \end{cases} \Leftrightarrow \begin{cases}
            \mu'_1 = \frac{\alpha}{\lambda} \\ \mu'_2 - \frac{\alpha^2}{\lambda^2} = \frac{\alpha}{\lambda^2}
        \end{cases} \Leftrightarrow \begin{cases}
            \mu'_1 = \frac{\alpha}{\lambda} \\ \mu'_2 = \frac{\alpha}{\lambda^2} + \frac{\alpha^2}{\lambda^2}
        \end{cases} \Leftrightarrow \begin{cases}
            \mu'_1 = \frac{\alpha}{\lambda} \\ \mu'_2 = \frac{\alpha + \alpha^2}{\lambda^2}
        \end{cases} \\
        & \Downarrow\\
        & \begin{cases}
            M'_1 = \frac{\alpha}{\lambda} \\ M'_2 = \frac{\alpha + \alpha^2}{\lambda^2}
        \end{cases} \Leftrightarrow \begin{cases}
            \alpha = M'_1\lambda \\ M'_2 = \frac{M'_1\lambda + (M'_1)^2\lambda^2}{\lambda^2}
        \end{cases} \Leftrightarrow (\cdots) \Leftrightarrow \begin{cases}
            \alpha = M'_1\lambda \\ \lambda = \frac{M'_1}{M'_2 - (M'_1)^2}
        \end{cases} \Leftrightarrow \begin{cases}
            \alpha = M'_1\lambda \\ \lambda = \frac{\bar{X}}{S^2}
        \end{cases} \Leftrightarrow \begin{cases}
            \alpha = \frac{\bar{X}^2}{S^2} \\ \lambda = \frac{\bar{X}}{S^2}
        \end{cases}
    \end{flalign*}
}

The \textbf{advantages} of using the method of momentes include 1) there is always a solution and 2) there is no need to assume much, just need the moments to exist.
The \textbf{disadvantage} arises when the support depends on the parameter as moment does not take into consideration of the support of the distribution.\\

Recalling the properties of the sampling moments 
\begin{flalign*}
    & E[M'_r|\theta] = \mu'_r\\
    & Var(M'_r|\theta) = \frac{[\mu'_{2r} - (\mu'_r)^2]}{n} \equiv \frac{\upsilon_{rr}}{n}\\
    & Cov_{\theta}(M'_r, M'_s) = \frac{[\mu'_{r+s} - \mu'_r\mu'_s]}{n} \equiv \frac{\upsilon_{rs}}{n}
\end{flalign*}
The multivariate \textbf{central limit theorem} states that, with $M = (M'_1,\ldots,M'_k)$ and $\mu = (\mu'_1,\ldots,\mu'_k)$,
\begin{equation*}
    \sqrt{n}(M'-\mu) \xrightarrow{d} N_k(0, V)
\end{equation*}
with $V = [\upsilon_{rs}]$. 
The method of moments estimator of $\theta$ is a function of of $M$, and its asymptotic properties can be determines using the \textbf{delta method}:
\begin{equation*}
    \sqrt{n}(h(M'_r) - h(\mu'_r))\xrightarrow{d} N(0, [h'(\mu_r)]^2\upsilon_{rr})
\end{equation*}

\subsection{Maximum likelihood estimator}

The idea is to propose an estimate of $\theta$ that \textbf{maximizes the likelihood function}, 
which measures how likely it is that $\theta$ gives the true value of the parameters that generated the observed data.
Formally, the MLE of $\theta$, when it exists, is $\hat{\theta}$ such that
\begin{equation*}
    L(\hat{\theta}|x_1,\ldots,x_n) \geq L(\theta|x_1,\ldots,x_n)\quad \forall \theta\in\Theta
\end{equation*}

In most cases, it is easier and equivalent to find the value of $\theta$ which maximizes the log-likelihood function.
This is done by finding the zeroes of the score function 
\begin{equation*}
    \frac{d}{d\theta}\ln L(\theta|x_1,\ldots,x_n) = 0
\end{equation*}
then verifying that the stationary point is indeed a global maximum.
This is done by finding the Hessian and proving it's negative (concave).
\begin{proof}
    We know that the likelihood function is always positive. And the function $\ln x$ is an increasing function.
    $\Rightarrow$ Value of $\theta$ that maximizes $L(\theta|\tdx)$ is the value of $\theta$ that maximizes $\ln L(\theta|\tdx)$
    \begin{equation*}
        L(\theta|\tdx) \propto \sum_{i=1}^{n} f(x_i|\theta) \Rightarrow \ln L(\theta|\tdx) = c + \sum_{i=1}^{n} \ln f(x_i|\theta)
    \end{equation*}
\end{proof}

The MLE may not be unique (case of Normal distribution), and in many cases there will not be closed form expression for the estimate, in which case we have to resort to numerical methods.

\ex[]{MLE : Poisson}{
    $X_1\ldots,X_n\sim Po(\lambda),\; \lambda>0$
    \begin{flalign*}
        & L(\lambda|\tdx) \propto e^{-n\lambda}\lambda^{\sum x_i} \Rightarrow \ln L = c - n\lambda + \sum x_i\ln \lambda\\
        & S(\lambda|\tdx) = \frac{\partial \ln L}{\partial \lambda} = -n + \frac{\sum x_i}{\lambda}\\
        & S(\lambda|\tdx) = 0 \Leftrightarrow  -n + \frac{\sum x_i}{\lambda} = 0 \Leftrightarrow \lambda = \bar{x}\\
        & \frac{\partial^2\ln L}{\partial \lambda^2} = \frac{-\sum x_i}{\lambda^2} < 0 \Rightarrow \text{maximum}\Rightarrow \hat{\lambda} = \bar{x}
    \end{flalign*}
}
\ex[]{MLE : Gamma}{
    $X_1\ldots,X_n\sim G(\alpha,\lambda),\; \lambda\text{ is known}$
    \begin{flalign*}
        & L(\alpha|\tdx) \propto \prod_{i=1}^{n}\frac{\lambda^\alpha}{\Gamma(\alpha)}x_i^{\alpha-1}e^{-\lambda} \propto \frac{\lambda^{n\alpha}}{(\Gamma(\alpha))^n}\left(\prod_{i=1}^{n}x_i\right)^\alpha\\
        & \ln L(\alpha|\tdx) = c(\lambda,\tdx) + n\alpha\ln\lambda - n\ln\Gamma(\alpha) + \alpha\sum\ln x_i\\
        & S(\alpha|\tdx) = n\ln\lambda - n\frac{\Gamma'(\alpha)}{\Gamma(\alpha)} + \sum\ln x_i\\
        & S(\alpha|\tdx) = 0 \Leftrightarrow n\ln\lambda - n\psi(\alpha) + \sum\ln x_i \Rightarrow \text{needs to be solved numerically}
    \end{flalign*}
}

\textbf{Properties} of the MLE:
\begin{itemize}
    \item \textbf{Invariance}: If $\hat{\theta}$ is the MLE of $\theta$, and $\tau$ is a one-to-one function of $\theta$, then $\tau(\hat{\theta})$ is the MLE of $\tau(\theta)$. 
    This can be generalized to situations where $\tau(\cdot)$ is not one-to-one as well, as long as it's reasonable.
    \item If $T$ is \textbf{sufficient} and there is a MLE of $\theta$, then this estimator is a function of $T$.
    \begin{proof}
        If sufficient then 
        \begin{flalign*}
            & L(\theta|\tdx) \propto f(\tdx|\theta) = g(T(\tdx);\theta)h(\tdx)\\
            & \ln L(\theta|\tdx) = c(\tdx) + \ln g(T(\tdx);\theta)\\
            & \frac{\partial\ln L}{\partial\theta} = \frac{\partial g(T(\tdx);\theta)}{\partial\theta} = 0
        \end{flalign*}
        The solution to this equation will be a function of $T(\tdx)$
    \end{proof}
    \item The \textbf{most efficient} estimator of $\theta$, if it exists, is also the MLE of $\theta$.
    \begin{proof}
        If there is a most efficient estimator, then 
        \begin{flalign*}
            & S(\theta|\tdx) = a(\theta)[T(\tdx) - \theta]\\
            & S(\theta|\tdx) = 0 \Leftrightarrow \theta = T(\tdx)
        \end{flalign*}
        The MLE is the most efficient estimator of $\theta$.
    \end{proof}
    \nt{
        If I've found the most efficient estimator, no need to find the MLE becuase they're the same.
        But finding the MLE first then saying it's the most efficient estimator is wrong.
    }
    \item Under regularity conditions, the MLE of $\theta$ satisfies
    \begin{equation*}
        \sqrt{n}(\hat{\theta}-\theta) \xrightarrow{d} N(0, [I_X(\theta)]^{-1})
    \end{equation*}
    It is possible to replace $I_{X_1,\ldots,X_n}(\theta)$ either by \begin{itemize}
        \item The Fisher information evaluated at MLE: $nI_X(\hat{\theta})$
        \item The observed Fisher information: $H(\theta) = -\frac{\partial^2}{\partial\theta^2}\ln L(\hat{\theta}|X_1,\ldots,X_n)$
    \end{itemize}
    \item Above results combined show that \textbf{MLE is BAN}( Best Asymptotically Normal) and is consistent and asymptotically most efficient.
    \begin{proof}
        Proof of consistency,
        \begin{flalign*}
            & \sqrt{n}(\hat{\theta}-\theta) \xrightarrow{d} N(0, [I_X(\theta)]^{-1})\\
            & \Downarrow a_n(T_n-\theta)\xrightarrow{d} T \Rightarrow T_n\xrightarrow{p}\theta\\
            & \hat{\theta}\xrightarrow{p} \theta \Rightarrow \hat{\theta} \text{ is consistent}
        \end{flalign*}
        Asymptotic variance of $\hat{\theta} = \frac{1}{nI_X(\theta)} =$ CRLB under regularity conditions.
        \begin{flalign*}
            & \ln L(\theta|\tdx) = \sum\ln f(x_i|\theta)\\
            & S(\theta|\tdx) = \sum S(\theta|X_i)\\
            & S(\theta|\tdx) = \underbrace{S(\hat{\theta}|\tdx)}_{=0} + (\hat{\theta} - \theta)\frac{\partial S}{\partial \theta} + R \Rightarrow\text{1st order Taylor Expansion}\\
            & S(\theta|\tdx) = (\hat{\theta} - \theta)\frac{\partial S}{\partial \theta}\\
            & \downarrow \text{sum of iid r.v.}\\
            & E[S(\theta|\tdx)] = 0\\
            & E\left[-\frac{\partial S}{\partial\theta}\right] = I_{\tdx}(\theta)
        \end{flalign*}
    \end{proof}
\end{itemize}




%\dfn{Definition Topic}{Definition Statement}
%\thm{Theorem Name}{Theorem Statement}
%\cor[cori]{Corollary Name}{Corollary Statement}
%\lem{Lemma Name}{Lemma Statement}
%\clm{Claim Name}{Claim Statement}
%\ex{Example Name}{Example explained}
%\opn{Open Question Name}{Question Statement}
%\pr{Question Name}{Question Statement}
%\nt{Special Note}
%\wc{Wrong Concept topic}{Explanation}
%\proof{Proof Idea}{}